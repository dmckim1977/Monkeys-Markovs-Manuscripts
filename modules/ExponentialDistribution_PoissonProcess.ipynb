{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exponential Distribution and Poisson Process\n",
    "- Ross: Chapters 5.1, 5.2, 5.3.1-5.3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 5 - The Exponential Distribution and the Poisson Process - Ross**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.1 Introduction**\n",
    "\n",
    "**Motivation:**\n",
    "- We must make enough simplifying assumptions to enable us to handle the mathematics but not so many that the mathematical model no longer resembles the real-world phenomenon\\\n",
    "- Common assumption that certain random variables are exponentially distributed\n",
    "  - exponential distribution is both relatively easy to work with and is often a good approximation to the actual distribution\n",
    "  - it does not deteriorate with time (no theta decay)\n",
    "    - an item that has been in use for ten (or any number of) hours is as good as a new item in regards to the amount of time remaining until the item fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.2 The Exponential Distribution**\n",
    "### ***5.2.1 Definition***\n",
    "\n",
    "A continuous random variable $X$ is said to have an exponential distribution with parameter $\\lambda$, $\\lambda > 0$, if its probability density function (PDF) is given by\n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases} \n",
    "\\lambda e^{-\\lambda x}, & \\text{if } x \\geq 0 \\\\\n",
    "0, & \\text{if } x < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "or, equivalently, if its cumulative distribution function (CDF) is given by\n",
    "$$\n",
    "F(x) = \\int_{-\\infty}^x f(y) \\, dy =\n",
    "\\begin{cases}\n",
    "1 - e^{-\\lambda x}, & \\text{if } x \\geq 0 \\\\\n",
    "0, & \\text{if } x < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The mean of the exponential distribution, $E[X]$, is given by\n",
    "$$\n",
    "E[X] = \\int_{-\\infty}^\\infty x f(x) \\, dx = \\int_0^\\infty \\lambda x e^{-\\lambda x} \\, dx\n",
    "$$\n",
    "\n",
    "Integrating by parts (with $u = x$ and $dv = \\lambda e^{-\\lambda x} \\, dx$) yields\n",
    "$$\n",
    "E[X] = -xe^{-\\lambda x} \\bigg|_0^\\infty + \\int_0^\\infty e^{-\\lambda x} \\, dx = \\frac{1}{\\lambda}\n",
    "$$\n",
    "\n",
    "The moment generating function (MGF) $\\phi(t)$ of the exponential distribution is given by\n",
    "$$\n",
    "\\phi(t) = E[e^{tX}] = \\int_0^\\infty e^{tx} \\lambda e^{-\\lambda x} \\, dx = \\frac{\\lambda}{\\lambda - t}\n",
    "$$\n",
    "for $t < \\lambda$ (Equation 5.1).\n",
    "\n",
    "All the moments of $X$ can now be obtained by differentiating Eq. (5.1). For example,\n",
    "$$\n",
    "E[X^2] = \\left. \\frac{d^2}{dt^2} \\phi(t) \\right|_{t=0} = \\left. \\frac{2\\lambda}{(\\lambda - t)^3} \\right|_{t=0} = \\frac{2}{\\lambda^2}\n",
    "$$\n",
    "\n",
    "Consequently, the variance $\\operatorname{Var}(X)$ is given by\n",
    "$$\n",
    "\\operatorname{Var}(X) = E[X^2] - (E[X])^2 = \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2} = \\frac{1}{\\lambda^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 5.1 (Exponential Random Variables and Expected Discounted Returns)**:\n",
    "\n",
    "Suppose that you are receiving rewards at randomly changing rates continuously throughout time. Let $R(x)$ denote the random rate at which you are receiving rewards at time $x$. For a value $\\alpha \\geq 0$, called the discount rate, the quantity\n",
    "$$\n",
    "R = \\int_0^\\infty e^{-\\alpha x} R(x) \\, dx\n",
    "$$\n",
    "represents the total discounted reward. (In certain applications, $\\alpha$ is a continuously compounded interest rate, and $R$ is the present value of the infinite flow of rewards.)\n",
    "\n",
    "The expected value of $R$, $E[R]$, is given by\n",
    "$$\n",
    "E[R] = E\\left[ \\int_0^\\infty e^{-\\alpha x} R(x) \\, dx \\right] = \\int_0^\\infty e^{-\\alpha x} E[R(x)] \\, dx\n",
    "$$\n",
    "\n",
    "- Therefore, the expected total discounted reward is equal to the expected total (undiscounted) reward earned by a random time that is exponentially distributed with a rate equal to the discount factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***5.2.2 Properties of the Exponential Distribution***\n",
    "\n",
    "A random variable X is said to be without memory or ***memoryless*** if\n",
    "\n",
    "$$\n",
    "P\\{X > s + t \\mid X > t\\} = P\\{X > s\\} \\;\\;\\; \\text{ for all s,t ≥ 0 }\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\implies\n",
    "$$\n",
    "\n",
    "$$\n",
    "P\\{X > s + t\\} = P\\{ X > s\\} P\\{X > t\\} \n",
    "$$\n",
    "\n",
    "- Think of X as being the lifetime of some instrument; ***memoryless*** tells us that the probability that the instrument lives for at least $s + t$ hours given that it has survived $t$ hours is the same as the initial probability that it lives for at least $s$ hours\n",
    "- if the instrument is alive at time $t$, then the distribution of the remaining amount of time that it survives is the same as the original lifetime distribution\n",
    "  - does not remember that it has already been in use for a time $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 5.2:** Suppose that the amount of time one spends in a bank is exponentially distributed with mean ten minutes, that is, $λ = \\frac{1}{10}$ . What is the probability that a customer will spend more than fifteen minutes in the bank? What is the probability that a customer will spend more than fifteen minutes in the bank given that she is still in the bank after ten minutes?\n",
    "\n",
    "$$\n",
    "P\\{X > 15\\} = e^{-15 \\lambda} = e^{-3/2} = 0.223\n",
    "$$\n",
    "\n",
    "$$\n",
    "P\\{X > 5\\} = e^{-5 \\lambda} = e^{-1/2} = 0.607\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 5.4**: The dollar amount of damage involved in an automobile accident is an exponential random variable with mean 1000. Of this, the insurance company only pays that amount exceeding (the deductible amount of) 400. Find the expected value and the standard deviation of the amount the insurance company pays per accident.\n",
    "\n",
    "$$\n",
    "I = \n",
    "\\begin{cases} \n",
    "1, & \\text{if } X > 400 \\\\\n",
    "0, & \\text{if } X \\leq 400\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Amount paid:\n",
    "$$\n",
    "Y =(X − 400)^{+} \\; \n",
    "$$\n",
    "\n",
    "$$\n",
    "E[Y \\mid I = 1] = 1000,\n",
    "$$\n",
    "$$\n",
    "E[Y \\mid I = 0] = 0,\n",
    "$$\n",
    "$$\n",
    "\\text{Var}(Y \\mid I = 1) = (1000)^2,\n",
    "$$\n",
    "$$\n",
    "\\text{Var}(Y \\mid I = 0) = 0.\n",
    "$$\n",
    "\n",
    "$$\n",
    "E[Y] = E[E[Y \\mid I]] = 103 E[I] = 103 e^{-0.4} \\approx 670.32,\n",
    "$$\n",
    "$$\n",
    "\\text{Var}(Y) = E[\\text{Var}(Y \\mid I)] + \\text{Var}(E[Y \\mid I]) = 10^6 e^{-0.4} + 10^6 e^{-0.4}(1 - e^{-0.4}) \\approx 944.09^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 5.5:** A store must decide how much of a certain commodity to order so as to meet next month’s demand, where that demand is assumed to have an exponential distribution with rate λ. If the commodity costs the store c per pound, and can be sold at a price of s>c per pound, how much should be ordered so as to maximize the store’s expected profit? Assume that any inventory left over at the end of the month is worthless and that there is no penalty if the store cannot meet all the demand\n",
    "\n",
    "profit:\n",
    "$$\n",
    "P = s \\min(X, t) - ct\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\min(X, t) = X - (X - t)^t\n",
    "$$\n",
    "\n",
    "upon conditioning whether X>t and then using the lack of memory property of the exponential:\n",
    "$$\n",
    "E[(X − t)^+] = E[(X − t)^+|X>t]P(X > t) + E[(X − t)^+|X ≤ t]P(X ≤ t) = E[(X − t)^+|X>t]e^{−λt} = \\frac{1}{\\lambda}e^{−λt}\n",
    "$$\n",
    "\n",
    "$$\n",
    "E[\\min(X,t)] = \\frac{1}{λ} − \\frac{1}{λ}e^{−λt}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\implies\n",
    "$$\n",
    "\n",
    "$$\n",
    "E[P] = \\frac{s}{λ} − \\frac{s}{λ}e^{−λt} − ct, \\; t = \\frac{1}{\\lambda}\\log(s/c)\n",
    "$$\n",
    "\n",
    "- suppose that:\n",
    "  - all unsold inventory can be returned for the amount r < $\\min(s,c)$ per pound\n",
    "  - penalty cost p per pound of unmet demand\n",
    "\n",
    "$$\n",
    "E[P] = \\frac{s}{λ} − \\frac{s}{λ}e^{−λt} − ct + rE[(t − X)^+] − pE[(X − t)^+]\n",
    "$$\n",
    "\n",
    "$$\n",
    "E[(t − X)^+] = t − E[\\min(X,t)] = t − \\frac{1}{λ} + \\frac{1}{λ}e^{−λt}\n",
    "$$\n",
    "\n",
    "$$\n",
    "E[P] = \\frac{s - r}{λ} − \\frac{r - s - p}{λ}e^{−λt} − (c - r)t, \\; t = \\frac{1}{\\lambda}\\log(\\frac{s + p - r}{c - r})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memoryless property is further illustrated by the failure rate function/hazard rate function of the exponential distribution. \\ \n",
    "Consider a continuous positive random variable X having distribution function $F$ and density $f$. The ***failure (or hazard) rate*** function $λ(t)$ is defined by:\n",
    "\n",
    "$$\n",
    "\\lambda(t) = \\frac{f(t)}{1 - F(t)}\n",
    "$$\n",
    "\n",
    "- Suppose that an item, having lifetime $X$, has survived for $t$ hours, and we desire the probability that it does not survive for an additional time $dt$\n",
    "$$\n",
    "P\\{X \\in (t,t + dt) \\mid X > t\\}  = \\frac{f(t)dt}{1 − F(t)} = λ(t)dt\n",
    "$$\n",
    "\n",
    "- $λ(t)$ represents the conditional probability density that a t-year-old item will fail\n",
    "\n",
    "<br>\n",
    "\n",
    "- Suppose now that the lifetime distribution is exponential. Then, by the memoryless property, it follows that the distribution of remaining life for a $t$-year-old item is the same as for a new item. Hence, $λ(t)$ should be constant:\n",
    "$$\n",
    "λ(t) = \\frac{\\lambda e^{-\\lambda t}}{e^{-\\lambda t}} = \\lambda\n",
    "$$\n",
    "- failure rate function for the exponential distribution is constant\n",
    "- prameter $λ$ is often referred to as the rate of the distribution\n",
    "- failure rate function $λ(t)$ uniquely determines the distribution $F$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 5.6**: Let $X_1, \\ldots, X_n$ be independent exponential random variables with respective rates $\\lambda_1, \\ldots, \\lambda_n$, where $\\lambda_i \\neq \\lambda_j$ when $i \\neq j$. Let $T$ be independent of these random variables and suppose that\n",
    "$$\n",
    "\\sum_{j=1}^n P_j = 1 \\text{ where } P_j = P\\{T = j\\}\n",
    "$$\n",
    "\n",
    "- random variable $X_T$ is said to be a ***hyperexponential random variable***\n",
    "\n",
    "- Distribution function $F$ of $X = X_T$, condition on $T$\n",
    "$$\n",
    "1 - F(t) = P\\{X > t\\} = \\sum_{i=1}^n P_i e^{-\\lambda_i t}\n",
    "$$\n",
    "$$\n",
    "f(t) = \\sum_{i=1}^n \\lambda_i P_i e^{-\\lambda_i t}\n",
    "$$\n",
    "\n",
    "- Failure rate function of a hyperexponential random variable:\n",
    "$$\n",
    "\\lambda(t) = \\frac{\\sum_{j=1}^n P_j \\lambda_j e^{-\\lambda_j t}}{\\sum_{i=1}^n P_i e^{-\\lambda_i t}} = \\sum_{j=1}^n \\lambda_j P\\{T = j \\mid X > t\\}\n",
    "$$\n",
    "\n",
    "- If $λ_1 < λ_i$, for all $i > 1$\n",
    "$$\n",
    "\\lim_{t \\to \\infty} P\\{T = 1 \\mid X > t\\} = 1\n",
    "$$\n",
    "\n",
    "- when $i \\ne 1$, \n",
    "$$\n",
    "\\lim_{t \\to \\infty} P\\{T = i \\mid X > t\\} = 0\n",
    "$$\n",
    "\n",
    "- above shows:\n",
    "$$\n",
    "\\lim_{t \\to \\infty}\\lambda(t) = \\min_i(\\lambda_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***5.2.3 Further Properties of the Exponential Distribution***\n",
    "\n",
    "**Proposition 5.1:** If $X_1, \\dots, X_n$ are independent exponential random variables with common rate $\\lambda$, then $\\sum_{i=1}^n X_i$ is a gamma $(n, λ)$ random variable. That is, its density function is:\n",
    "$$\n",
    "f(t) = \\lambda e^{-\\lambda t} \\frac{(\\lambda t)^{n-1}}{(n-1)!}, \\quad t > 0\n",
    "$$\n",
    "\n",
    "\n",
    "To determine the probability that one exponential random variable is smaller than another. That is, suppose that $X_1$ and $X_2$ are independent exponential random variables with respective means $1/λ_1$ and $1/λ_2$; what is $P\\{X_1 < X_2\\}$\n",
    "$$\n",
    "P\\{X_1 < X_2\\} = \\int_0^\\infty P\\{X_1 < X_2 \\mid X_1 = x\\} \\lambda_1 e^{-\\lambda_1 x} \\, dx = \\int_0^\\infty \\lambda_1 e^{-(\\lambda_1 + \\lambda_2)x} \\, dx\n",
    "$$\n",
    "$$\n",
    "= \\frac{\\lambda_1}{\\lambda_1 + \\lambda_2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that $X_1,X_2, \\dots, X_n$ are independent exponential random variables, with $X_i$ having rate $μ_i$, $i = 1, \\dots, n$. It turns out that the smallest of the $X_i$ is exponential with a rate equal to the sum of the $μ_i$.\n",
    "\n",
    "$$\n",
    "P\\{\\min(X_1, \\ldots, X_n) > x\\} = P\\{X_i > x \\text{ for each } i = 1, \\ldots, n\\}\n",
    "$$\n",
    "$$\n",
    "= \\prod_{i=1}^n e^{-\\mu_i x} \n",
    "$$\n",
    "$$\n",
    "= \\exp \\Biggl\\{- \\left(\\sum_{i=1}^n \\mu_i \\right)x \\Biggr\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposition 5.2:**. If $X_1, \\dots, X_n$ are independent exponential random variables with respective rates $λ_1, \\dots, λ_n$, then $\\min_i(X_i)$ is exponential with rate $\\sum_{i = 1}^n λ_i$ Further, $\\min_i(X_i)$ and the rank order of the variables $X_1,X_2, \\dots, X_n$ are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 5.10:**. Suppose that customers are in line to receive service that is provided sequentially by a server; whenever a service is completed, the next person in line enters the service facility. However, each waiting customer will only wait an exponentially distributed time with rate $\\theta$; if its service has not yet begun by this time then it will immediately depart the system. These exponential times, one for each waiting customer, are independent. In addition, the service times are independent exponential random variables with rate $\\mu$. Suppose that someone is presently being served and consider the person who is nth in line.\n",
    ">> (a) Find $P_n$, the probability that this customer is eventually served. \\\n",
    ">> (b) Find $W_n$, the conditional expected amount of time this person spends waiting in line given that she is eventually served\n",
    "\n",
    "-  smallest of these n+1 independent exponentials is the departure time of the nth person in line, the conditional probability that this person will be served is 0\n",
    "- given that this person’s departure time is not the smallest, the conditional probability that this person will be served is the same as if it were initially in position n − 1\n",
    "\n",
    "a:\n",
    "$$\n",
    "P_n, (n+1) = \\frac{(n - 1)\\theta + \\mu}{n\\theta + \\mu} P_{n-1}\n",
    "$$\n",
    "$$\n",
    "P_n, (n-1) = \\frac{(n - 2)\\theta + \\mu}{n\\theta + \\mu} P_{n-2}\n",
    "$$\n",
    "$$\n",
    "P_{n} = \\frac{\\theta + \\mu}{n\\theta + \\mu} P_{n_1} = \\frac{\\mu}{\\theta + \\mu}\n",
    "$$\n",
    "\n",
    "b:\n",
    "- use the fact that the minimum of independent exponentials is, independent of their rank ordering, exponential with a rate equal to the sum of the rates\n",
    "- the time until the nth person in line enters service is the minimum of these n + 1 random variables plus the additional time thereafter\n",
    "\n",
    "$$\n",
    "W_n = \\frac{1}{n\\theta + \\mu} + W_{n-1}\n",
    "$$\n",
    "$$\n",
    "W_n = \\sum_{i+1}^n \\frac{1}{i\\theta + \\mu} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***5.2.4 Convolutions of Exponential Random Variables***\n",
    "\n",
    "Let $X_i, i = 1, \\dots, n$ are independent exponential random variables with respective rates $λ_i, i = 1, \\dots, n$, and suppose that $\\lambda_i \\ne \\lambda_j$ for $i \\ne j$. The random variable $\\sum_{i=1}^n X_i$ is said to be a hypoexponential random variable. \\\n",
    "To compute its probability density function, n = 2 case: \n",
    "\n",
    "$$\n",
    "f_{X_1+X_2}(t) = \\int_0^t f_{X_1}(s) f_{X_2}(t-s) \\, ds = \\int_0^t \\lambda_1 e^{-\\lambda_1 s} \\lambda_2 e^{-\\lambda_2 (t-s)} \\, ds\n",
    "$$\n",
    "$$\n",
    "= \\frac{\\lambda_1}{\\lambda_1 - \\lambda_2} \\lambda_2 e^{-\\lambda_2 t} \\left(1 - e^{-(\\lambda_1 - \\lambda_2) t}\\right)\n",
    "$$\n",
    "$$\n",
    "= \\frac{\\lambda_1}{\\lambda_1 - \\lambda_2} \\lambda_2 e^{-\\lambda_2 t} + \\frac{\\lambda_2}{\\lambda_2 - \\lambda_1} \\lambda_1 e^{-\\lambda_1 t}\n",
    "$$\n",
    "\n",
    "For $n = 3$, the PDF is given by:\n",
    "$$\n",
    "f_{X_1+X_2+X_3}(t) = \\sum_{i=1}^3 \\lambda_i e^{-\\lambda_i t} \\left(\\prod_{j \\neq i} \\frac{\\lambda_j}{\\lambda_j - \\lambda_i}\\right)\n",
    "$$\n",
    "\n",
    "General result for any $n$:\n",
    "$$\n",
    "f_{X_1+\\cdots+X_n}(t) = \\sum_{i=1}^n C_{i,n} \\lambda_i e^{-\\lambda_i t}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "C_{i,n} = \\prod_{j \\neq i} \\frac{\\lambda_j}{\\lambda_j - \\lambda_i}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 5.11. Let $X_1, \\dots, X_m$ be independent exponential random variables with respective rates $λ_1, \\dots, λ_m$, where $λ_i \\ne λ_j$ when $i \\ne j$. Let N be independent of these random variables and suppose that $\\sum_{n=1}^1 P_n = 1$, where $P_n = P\\{N = n\\}$. The random variable: \n",
    "\n",
    "$$\n",
    "Y = \\sum_{j = 1}^N X_i\n",
    "$$\n",
    "\n",
    "is said to be a ***Coxian random variable***. \\\n",
    "Conditioning on N gives its density function:\n",
    "\n",
    "$$\n",
    "f_Y(t) = \\sum_{n=1}^m f_Y(t \\mid N = n) P_n = \\sum_{n=1}^m P_n \\left(\\sum_{i=1}^n C_{i,n} \\lambda_i e^{-\\lambda_i t}\\right)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "If we interpret $N$ as a lifetime measured in discrete time periods, then $r(n)$ denotes the probability that an item will die in its nth period of use given that it has survived up to that time. Thus, $r(n)$ is the discrete time analog of the failure rate function $λ(t)$, and is correspondingly referred to as the discrete time failure (or hazard) rate function.\n",
    "\n",
    "$$\n",
    "r(n) = P\\{N = n \\mid N \\geq n\\}\n",
    "$$\n",
    "\n",
    "- Suppose that an item must go through m stages of treatment to be cured.\n",
    "- However, suppose that after each stage there is a probability that the item will quit the program\\\n",
    "- the probability that an item that has just completed stage $n$ quits the program is (independent of how long it took to go through the $n$ stages) equal to $r(n)$\n",
    "-  the total time that an item spends in the program is a Coxian random variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***5.2.5 The Dirichlet Distribution***\n",
    "\n",
    "Consider an experiment with possible outcomes $1, 2, \\ldots, n$, having respective probabilities $P_1, \\ldots, P_n$, such that $\\sum_{i=1}^n P_i = 1$. We assume a probability distribution on the vector $(P_1, \\ldots, P_n)$. Because $\\sum_{i=1}^n P_i = 1$, we cannot define a density on $P_1, \\ldots, P_n$, but what we can do is to define one on $P_1, \\ldots, P_{n-1}$ and then take $P_n = 1 - \\sum_{i=1}^{n-1} P_i$. The Dirichlet distribution assumes that $(P_1, \\ldots, P_{n-1})$ is uniformly distributed over the set $S = \\{(p_1, \\ldots, p_{n-1}) : \\sum_{i=1}^{n-1} p_i < 1, 0 < p_i, i = 1, \\ldots, n-1\\}$. Thus, the Dirichlet joint density function is \n",
    "$$\n",
    "f_{P_1, \\ldots, P_{n-1}}(p_1, \\ldots, p_{n-1}) = C, \\text{ for } 0 < p_i, \\text{ where } i = 1, \\ldots, n-1, \\text{ and } \\sum_{i=1}^{n-1} p_i < 1\n",
    "$$\n",
    "Because integrating the preceding density over the set $S$ yields that\n",
    "$$\n",
    "1 = C \\Pr(U_1 + \\ldots + U_{n-1} < 1)\n",
    "$$\n",
    "where $U_1, \\ldots, U_{n-1}$ are independent uniform (0, 1) random variables.\n",
    "\n",
    "\n",
    "**Proposition 5.3.** Let $X_1, \\ldots, X_n$ be independent exponential random variables with rate $\\lambda$, and let $S = \\sum_{i=1}^n X_i$. Then, $\\left( \\frac{X_1}{S}, \\frac{X_2}{S}, \\ldots, \\frac{X_{n-1}}{S} \\right)$ has a Dirichlet distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.3 The Poisson Process**\n",
    "### ***5.3.1 Counting Processes***\n",
    "\n",
    "A stochastic process $\\{N(t),t \\geq 0\\}$ is said to be a counting process if $N(t)$ represents the total number of “events” that occur by time $t$. \n",
    "- a: If we let $N(t)$ equal the number of persons who enter a particular store at or prior to time t, then $\\{N(t),t ≥ 0\\}$ is a counting process in which an event corresponds to a person entering the store. Note that if we had let $N(t)$ equal the number of persons in the store at time $t$, then $\\{N(t),t ≥ 0\\}$ would not be a counting process (why not?).\n",
    "- b: If we say that an event occurs whenever a child is born, then $\\{N(t),t ≥ 0\\}$ is a counting process when $N(t)$ equals the total number of people who were born by time $t$. (Does $N(t)$ include persons who have died by time $t$? Explain why it must.)\n",
    "- c: If $N(t)$ equals the number of goals that a given soccer player scores by time $t$, then $\\{N(t),t \\geq 0\\}$ is a counting process. An event of this process will occur whenever the soccer player scores a goal.\n",
    "\n",
    "<br>\n",
    "\n",
    "For a counting process $N(t)$, must satisfy:\n",
    "- $N(t) > 0$\n",
    "- $N(t)$ is integer valued\n",
    "- If $s < t$, then $N(s) \\leq N(t)$\n",
    "- For $s < t$, $N(t) - N(s)$ equals the number of events that occur in the interval $(s,t]$\n",
    "\n",
    "<br>\n",
    "\n",
    "- A counting process is said to possess independent increments if the numbers of events that occur in disjoint time intervals are independent\n",
    "  - number of events that occur by time 10 ($N(10)$) must be independent of the number of events that occur between times 10 and 15 ($N(15) - N(10)$)\n",
    "  - assuming independence is case by case >> reasonable for *a* not *b* \n",
    "- A counting process is said to possess ***stationary increments*** if the distribution of the number of events that occur in any interval of time depends only on the length of the time interval\n",
    "  - the process has stationary increments if the number of events in the interval $(s,s + t)$ has the same distribution for all $s$\n",
    "  - example *a* can have reasonable assumption of stationary increments (if there were no times of day at which people were more likely to enter the store i.e no rush hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***5.3.2 Definition of the Poisson Process***\n",
    "\n",
    "**Definition 5.1:** The function $f(·)$ is said to be $o(h)$ if\n",
    "\n",
    "$$\n",
    "\\lim_{h \\rightarrow 0} \\frac{f(h)}{h} = 0\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Example 5.12:**\n",
    "\n",
    "> a) function $f(x) = x^2$ is $o(h)$\n",
    "\n",
    "$$\n",
    "\\lim_{h \\rightarrow 0} \\frac{f(h)}{h} = \\lim_{h \\rightarrow 0} \\frac{h^2}{h} = \\lim_{h \\rightarrow 0} h = 0\n",
    "$$\n",
    "\n",
    "> b) function $f(x) = x$ is not $o(h)$\n",
    "\n",
    "$$\n",
    "\\lim_{h \\rightarrow 0} \\frac{f(h)}{h} = \\lim_{h \\rightarrow 0} \\frac{h}{h} = \\lim_{h \\rightarrow 0} 1 \\ne 0\n",
    "$$\n",
    "\n",
    "> c) $f(·)$ is $o(h)$ and $g(·)$ is $o(h)$, then so is $f (·) + g(·)$ \\\n",
    "> d) If $f(·)$ is $o(h$), then so is $g(·) = cf(·)$ \\\n",
    "> e) any finite linear combination of functions, each of which is $o(h)$, is $o(h)$\n",
    "\n",
    "The $o(h)$ notation can be used to make statements more precise. For instance, if $X$ is continuous with density $f$ and failure rate function $\\lambda(t)$, then the approximate statements\n",
    "$$\n",
    "P(t < X < t + h) \\approx f(t)h\n",
    "$$\n",
    "$$\n",
    "P(t < X < t + h \\mid X > t) \\approx \\lambda(t)h\n",
    "$$\n",
    "can be precisely expressed as\n",
    "$$\n",
    "P(t < X < t + h) = f(t)h + o(h)\n",
    "$$\n",
    "$$\n",
    "P(t < X < t + h \\mid X > t) = \\lambda(t)h + o(h)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition 5.2**: The counting process $\\{N(t), t \\geq 0\\}$ is said to be a Poisson process with rate $\\lambda > 0$ if the following axioms hold:\n",
    "1. $N(0) = 0$\n",
    "2. $\\{N(t), t \\geq 0\\}$ has independent increments\n",
    "3. $P(N(t + h) - N(t) = 1) = \\lambda h + o(h)$\n",
    "4. $P(N(t + h) - N(t) \\geq 2) = o(h)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma 5.1:** $\\{Ns(t),t ≥ 0\\}$ is a Poisson process with rate $λ$ \\\n",
    "**Lemma 5.2:** If $T_1$ is the time of the first event of the Poisson process $\\{N(t),t ≥ 0\\}$, then $P(T_1 > t) = P(N(t) = 0) = e^{−λt}$ \\\n",
    "**Proposition 5.4:** $T_1,T_2, \\dots$ are independent and identically distributed exponential random variables with rate $λ$ \n",
    "\n",
    "<br>\n",
    "\n",
    "Another quantity of interest is $S_n$, the time of the $n$-th event. Because the interarrival times are the times between successive events, it is easily seen that\n",
    "$$\n",
    "S_n = \\sum_{i=1}^n T_i, \\quad n \\geq 1\n",
    "$$\n",
    "Thus, from Propositions 5.4 and 5.1, it follows that $S_n$ is a gamma $(n, \\lambda)$ random variable with density function\n",
    "$$\n",
    "f_{S_n}(s) = \\lambda e^{-\\lambda s} (\\lambda s)^{n-1} \\frac{1}{(n-1)!}, \\quad s > 0\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Theorem 5.1:** If $\\{N(t),t \\geq 0\\} is a Poisson process with rate $λ$, then $N(t)$ is a Poisson random variable with rate $λt$. That is, \n",
    "$$\n",
    "P(N(t) = n) = e^{−λt}(λt)n/n!, \\quad n ≥ 0\n",
    "$$\n",
    "\n",
    "- A counting process for which the distribution of the number of events in an interval depends only on the length of the interval and not its location is said to have stationary increments. Thus, a Poisson process has stationary increments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 5.13:** Suppose that people immigrate into a territory according to a Poisson process with rate λ = 2 per day\n",
    "> a) Find the probability there are 10 arrivals in the following week (of 7 days)\n",
    "> b) Find the expected number of days until there have been 20 arrivals\n",
    "\n",
    "<br>\n",
    "\n",
    "- a: number of arrivals in 7 days is Poisson with mean $7 \\lambda = 14$,  probability there will be 10 arrivals is $e^{−14}(14)^{10}/10!$\n",
    "- b: $E[S_{20}] = 20/λ = 10$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***5.3.3 Further Properties of Poisson Processes***\n",
    "\n",
    "Consider a Poisson process $\\{N(t), t \\geq 0\\}$ having rate $\\lambda$, and suppose that each time an event occurs, it is classified as either a type I or a type II event. Suppose further that each event is classified as a type I event with probability $p$ or a type II event with probability $1-p$, independently of all other events. For example, suppose that customers arrive at a store in accordance with a Poisson process having rate $\\lambda$; and suppose that each arrival is male with probability $\\frac{1}{2}$ and female with probability $\\frac{1}{2}$. Then a type I event would correspond to a male arrival and a type II event to a female arrival.\n",
    "\n",
    "Let $N_1(t)$ and $N_2(t)$ denote respectively the number of type I and type II events occurring in $[0, t]$. Note that $N(t) = N_1(t) + N_2(t)$.\n",
    "\n",
    "**Proposition 5.5:**  $\\{N_1(t), t \\geq 0\\}$ and  $\\{N_2(t), t \\geq 0\\}$ are both Poisson processes having respective rates $λp$ and $λ(1 − p)$. Furthermore, the two processes are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 5.14:**. If immigrants to area A arrive at a Poisson rate of ten per week, and if each immigrant is of English descent with probability 1/12 , then what is the probability that no people of English descent will emigrate to area A during the month of February?\n",
    "\n",
    ">> the number of Englishmen emigrating to area A during the month of February is Poisson distributed with mean $4 \\times 10 \\times \\frac{1}{12} = \\frac{10}{3}$ $\\implies$ probability is $e^{-10/3}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 5.15:** Suppose nonnegative offers to buy an item that you want to sell arrive according to a Poisson process with rate λ. Assume that each offer is the value of a continuous random variable having density function f (x). Once the offer is presented to you, you must either accept it or reject it and wait for the next offer. We suppose that you incur costs at a rate c per unit time until the item is sold, and that your objective is to maximize your expected total return, where the total return is equal to the amount received minus the total cost incurred. Suppose you employ the policy of accepting the first offer that is greater than some specified value y. (Such a type of policy, which we call a y-policy, can be shown to be optimal.) What is the best value of y? What is the maximal expected net return?\n",
    "\n",
    "- $X$ is value of random offer\n",
    "- $\\bar{F} = P\\{X > x\\} = \\int_{x}^{\\infty} f(u) du$ is $X$'s tail distribution function\n",
    "- each offer will be greater than y with probability $\\bar{F}(y) \\implies$ Poisson process with rate $λ\\bar{F}(y)$\n",
    "  - the time until an offer is accepted is an exponential random variable with rate $λ\\bar{F}(y)$\n",
    "- $R(y)$ denote the total return from the policy that accepts the first offer that is greater than $y$\n",
    "\n",
    "$$\n",
    "E[R(y)] = E[\\text{accepted offer}] - cE[\\text{time to accept}]\n",
    "= E[X \\mid X > y] - \\frac{c}{\\lambda \\overline{F}(y)}\n",
    "$$\n",
    "$$\n",
    "= \\int_0^\\infty x f_{X \\mid X > y}(x) \\, dx - \\frac{c}{\\lambda \\overline{F}(y)}\n",
    "$$\n",
    "$$\n",
    "= \\int_y^\\infty \\frac{x f(x)}{\\overline{F}(y)} \\, dx - \\frac{c}{\\lambda \\overline{F}(y)}\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{\\overline{F}(y)} \\left(\\int_y^\\infty x f(x) \\, dx - \\frac{c}{\\lambda} \\right)\n",
    "$$\n",
    "$$\n",
    "\\frac{d}{dy}E[R(y)] = 0 \\implies \\text{the optimal value of y satisfies} \\quad y \\bar{F}(y) = \\int_{y}^{\\infty} x f(x) dx - \\frac{c}{\\lambda}\n",
    "$$\n",
    "\n",
    "Hence, the optimal policy is the one that accepts the first offer that is greater than $y^∗$, where $y^∗$ is such that\n",
    "$$\n",
    "\\int_{y^*}^{\\infty} (x - y^*) f(x) dx = \\frac{c}{\\lambda}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\implies\n",
    "$$\n",
    "\n",
    "$$\n",
    "E[R(y^*)] = y^*\n",
    "$$\n",
    "\n",
    "- the optimal critical value is also the maximal expected net return\n",
    "- note that when an offer is rejected the problem basically starts anew and so the maximal expected additional net return from then on is the maximal expected net return\n",
    "  - implies that it is optimal to accept an offer if and only if it is at least as large as the maximal expected additional net return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 5.17 (The Coupon Collecting Problem) [wiki link](https://en.wikipedia.org/wiki/Coupon_collector%27s_problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability that n events occur in one Poisson process before m events have occurred in a second and independent Poisson process: \n",
    "\n",
    "Let $\\{N1(t),t \\geq 0\\}$ and $\\{N2(t),t \\geq 0\\}$ be two independent Poisson processes having respective rates $λ_1$ and $λ_2$. Also, let $S_n^1$ denote the time of the $n$-th event of the first process, and $S_2^m$ the time of the $m$-th event of the second process:\n",
    "\n",
    "$$\n",
    "P\\{S_n^1 < S_m^2 \\}\n",
    "$$\n",
    "\n",
    "n = m = 1 case:\n",
    "\n",
    "$$\n",
    "P\\{S_1^1 < S_1^2 \\} = \\frac{\\lambda_1}{\\lambda_1 + \\lambda_2}\n",
    "$$\n",
    "\n",
    "two events occur in the $N_1(t)$ process before a single event has occurred in the $N_2(t)$ process case:\n",
    "\n",
    "$$\n",
    "P\\{S_2^1 < S_1^2 \\} = \\left(\\frac{\\lambda_1}{\\lambda_1 + \\lambda_2} \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each event that occurs is going to be an event of the $N_1(t)$ process with probability $λ_1/(λ_1 + λ_2)$ or an event of the $N_2(t)$ process with probability $λ_2/(λ_1 + λ_2$), independent of all that has previously occurred. \\\n",
    "The probability that the $N_1(t)$ process reaches $n$ before the $N_2(t)$ process reaches $m$ is just the probability that $n$ heads will appear before $m$ tails if one flips a coin having probability $p = λ_1/(λ_1 + λ_2)$ of a head appearing\n",
    "\n",
    "$$\n",
    "P\\{S_n^1 < S_m^2 \\} = \\sum_{k=n}^{n+m-1} \\binom{n+m-1}{k} \\left(\\frac{\\lambda_1}{\\lambda_1 + \\lambda_2}\\right)^k \\left(\\frac{\\lambda_2}{\\lambda_1 + \\lambda_2}\\right)^{n+m-1-k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***5.3.4 Conditional Distribution of the Arrival Times***\n",
    "\n",
    "Suppose we are told that exactly one event of a Poisson process has taken place by time $t$, and we are asked to determine the distribution of the time at which the event occurred. Now, since a Poisson process possesses stationary and independent increments it seems reasonable that each interval in $[0,t]$ of equal length should have the same probability of containing the event. In other words, the time of the event should be uniformly distributed over $[0,t]$\n",
    "\n",
    "$$\n",
    "P\\{T_1 < s \\mid N(t) = 1\\} = \\frac{s}{t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $Y_1, Y_2, \\ldots, Y_n$ be $n$ random variables. We say that $Y_{(1)}, Y_{(2)}, \\ldots, Y_{(n)}$ are the order statistics corresponding to $Y_1, Y_2, \\ldots, Y_n$ if $Y_{(k)}$ is the $k$-th smallest value among $Y_1, \\ldots, Y_n$, $k = 1, 2, \\ldots, n$. For instance, if $n = 3$ and $Y_1 = 4$, $Y_2 = 5$, $Y_3 = 1$, then $Y_{(1)} = 1$, $Y_{(2)} = 4$, $Y_{(3)} = 5$. If the $Y_i$, $i = 1, \\ldots, n$, are independent identically distributed continuous random variables with probability density $f$, then the joint density of the order statistics $Y^{(1)}, Y^{(2)}, \\ldots, Y^{(n)}$ is given by\n",
    "$$\n",
    "f(y_1, y_2, \\ldots, y_n) = n! \\prod_{i=1}^n f(y_i), \\text{ where } y_1 < y_2 < \\ldots < y_n\n",
    "$$\n",
    "The preceding follows since:\n",
    "1. $(Y^{(1)}, Y^{(2)}, \\ldots, Y^{(n)})$ will equal $(y_1, y_2, \\ldots, y_n)$ if $(Y_1, Y_2, \\ldots, Y_n)$ is equal to any of the $n!$ permutations of $(y_1, y_2, \\ldots, y_n)$;\n",
    "2. The probability density that $(Y_1, Y_2, \\ldots, Y_n)$ is equal to $(y_{i1}, \\ldots, y_{in})$ is $\\prod_{j=1}^n f(y_{ij}) = \\prod_{j=1}^n f(y_j)$ when $i1, \\ldots, in$ is a permutation of $1, 2, \\ldots, n$.\n",
    "\n",
    "If the $Y_i$, $i = 1, \\ldots, n$, are uniformly distributed over $(0, t)$, then we obtain from the preceding that the joint density function of the order statistics $Y^{(1)}, Y^{(2)}, \\ldots, Y^{(n)}$ is\n",
    "$$\n",
    "f(y_1, y_2, \\ldots, y_n) = \\frac{n!}{t^n}, \\text{ for } 0 < y_1 < y_2 < \\ldots < y_n < t\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Theorem 5.2:***. Given that $N(t) = n$, the $n$ arrival times $S_1, \\dots, S_n$ have the same distribution as the order statistics corresponding to $n$ independent random variables uniformly distributed on the interval $(0,t)$\n",
    "- Suppose that there are $k$ possible types of events and that the probability that an event is classified as a type $i$ event, $i = 1,\\dots,k$, depends on the time the event occurs. Specifically, suppose that if an event occurs at time $y$ then it will be classified as a type $i$ event, independently of anything that has previously occurred, with probability $P_i(y), i = 1, \\dots, k$ where $\\sum_{i=1}^k P_i(y) = 1$. Upon using above Theorem 5.2, we can prove the following useful proposition:\n",
    "\n",
    "**Proposition 5.6:** If $N_i(t), i =1, \\dots, k$, represents the number of type $i$ events occurring by time $t$ then $N_i(t), i = 1, \\dots, k$, are independent Poisson random variables having means:\n",
    "$$\n",
    "E[N_i(t)] = λ \\int_0^t P_i(s)ds\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 5.18:** (An Infinite Server Queue). Suppose that customers arrive at a service station in accordance with a Poisson process with rate $λ$. Upon arrival the customer is immediately served by one of an infinite number of possible servers, and the service times are assumed to be independent with a common distribution $G$. What is the distribution of $X(t)$, the number of customers that have completed service by time $t$? What is the distribution of $Y(t)$, the number of customers that are being served at time $t$? \n",
    "\n",
    "- type I: entering customer with completed service by time t >> service time is less than $t − s$ >> so has probability $G(t - s)$\n",
    "- type II: entering customer with incompleted service by time t >> service time is $s \\leq t$ with probability $\\bar{G}(t -s) = 1 - G(t - s)$\n",
    "\n",
    "From Prop 5.6, the distribution of $X(t)$, the number of customers that have completed service by time $t$, is Poisson distributed with mean:\n",
    "\n",
    "$$\n",
    "E[X(t)] = λ \\int_0^t G(t − s) ds = λ \\int_0^t G(y)dy\n",
    "$$\n",
    "\n",
    "The distribution of $Y(t)$, the number of customers being served at time $t$ is Poisson with mean: \n",
    "\n",
    "$$\n",
    "E[Y(t)] = λ \\int_0^t \\bar{G}(t − s) ds = λ \\int_0^t \\bar{G}(y)dy\n",
    "$$\n",
    "\n",
    "- $X(t)$ and $Y(t)$ are independent\n",
    "\n",
    "\n",
    "Suppose now that we are interested in computing the joint distribution of $Y(t)$ and $Y(t + s)$ — that is, the joint distribution of the number in the system at time $t$ and at time $t + s$. To accomplish this, say that an arrival is\n",
    "- type 1: if he arrives before time t and completes service between t and t + s,\n",
    "- type 2: if he arrives before t and completes service after t + s,\n",
    "- type 3: if he arrives between t and t + s and completes service after t + s,\n",
    "- type 4: otherwise\n",
    "\n",
    "Hence, an arrival at time $y$ will be type $i$ with probability $P_i(y)$ given by:\n",
    "$$\n",
    "P_1(y) = \n",
    "\\begin{cases} \n",
    "G(t + s - y) - G(t - y), & \\text{if } y < t \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_2(y) = \n",
    "\\begin{cases} \n",
    "\\bar{G}({t} + s - y), & \\text{if } y < t \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_3(y) = \n",
    "\\begin{cases} \n",
    "\\bar{G}({t} + s - y), & \\text{if } t < y < t + s \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_4(y) = 1 - P_1(y) - P_2(y) - P_3(y)\n",
    "$$\n",
    "\n",
    "\n",
    "From Prop 5.6, $N_i = N_i(s + t), i = 1, 2, 3$ are independent Poisson random variableswith respective means:\n",
    "$$\n",
    "E[N_i] = λ \\int_0^{t+s} P_i(y)dy, \\quad i = 1, 2, 3\n",
    "$$\n",
    "\n",
    "- $Y(t) = N_1 + N_2$\n",
    "- $Y(t + s) = N_2 + N_3$\n",
    "\n",
    "\n",
    "Joint distribution of Y(t) and Y(t + s):\n",
    "$$\n",
    "Cov[Y(t),Y(t + s)]\n",
    "$$\n",
    "$$\n",
    "= Cov(N_1 + N_2,N_2 + N_3)\n",
    "$$\n",
    "$$\n",
    "= Cov(N_2, N_2) \n",
    "$$\n",
    "- by independence of $N_1, N_2, N_3$\n",
    "$$\n",
    "= Var(N_2)\n",
    "$$\n",
    "$$\n",
    "λ \\int_0^t \\bar{G}(t + s − y)dy = λ \\int_0^t \\bar{G}(u + s) du\n",
    "$$\n",
    "- since the variance of a Poisson random variable equals its mean\n",
    "\n",
    "\n",
    "The joint distribution of $Y(t)$ and $Y(t + s)$ is as follows:\n",
    "$$\n",
    "P\\{Y(t) = i, Y(t + s) = j\\} = P\\{N_1 + N_2 = i, N_2 + N_3 = j\\}\n",
    "$$\n",
    "$$\n",
    "= \\sum_{l=0}^{\\min(i,j)} P\\{N_2 = l, N_1 = i - l, N_3 = j - l\\}\n",
    "$$\n",
    "$$\n",
    "= \\sum_{l=0}^{\\min(i,j)} P\\{N_2 = l\\} P\\{N_1 = i - l\\} P\\{N_3 = j - l\\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $S_n$, the time of the $n$-th event, then the first $n − 1$ event times are distributed as the ordered values of a set of $n − 1$ random variables uniformly distributed on $(0,Sn)$\n",
    "\n",
    "**Proposition 5.7.** Given that $S_n = t$, the set $S_1, \\ldots, S_{n-1}$ has the distribution of a set of $n - 1$ independent uniform $(0, t)$ random variables.\n",
    "\n",
    "**Proof.** We can prove the preceding in the same manner as we did for Theorem 5.2, or we can argue more loosely as follows:\n",
    "$$\n",
    "S_1, \\ldots, S_{n-1} \\mid S_n = t \\sim S_1, \\ldots, S_{n-1} \\mid S_n = t, N(t^-) = n - 1 \\sim S_1, \\ldots, S_{n-1} \\mid N(t^-) = n - 1\n",
    "$$\n",
    "where $\\sim$ means \"has the same distribution as\" and $t^-$ is infinitesimally smaller than $t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
