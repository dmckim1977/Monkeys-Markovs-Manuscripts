{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete-time Markov chains\n",
    "- Feldman/Valdez-Flores: Chapter 5\n",
    "- Ross: Chapters 4.1-4.4, 4.5.1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 5 - Feldman/Valdez-Flores**\n",
    "- Markov chain has the property that the future is independent of the past given the present\n",
    "- Named after the probabilist A. A. Markov who published a series of papers starting in 1907 which laid the theoretical foundations for finite state Markov chains\n",
    "- Galton-Watson process (when and with what probability would a given family name become extinct) is a Markov chain is the fact that at any generation, the number of individuals in the next generation is completely independent of the number of individuals in previous generations as long as the number of individuals in the current generation are known"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.1 Basic Definitions**\n",
    "-  Markov chain is a discrete parameter process in which the future is independent of the past given the present\n",
    "- High level example:\n",
    "  - play a game with a fair, unbiased coin, each start with five pennies and repeatedly toss the coin\n",
    "  - heads, then you give me a penny; tails you give me a penny\n",
    "  - continue until one of us has none and the other has ten pennies\n",
    "  - sequence of heads and tails from the successive tosses of the coin would form an i.i.d. stochastic process \n",
    "    - the sequence representing the total number of pennies that you hold would be a Markov chain\n",
    "    - assume that after several tosses, you currently have three pennies\n",
    "    - probability that after the next toss you will have four pennies is 0.5 and knowledge of the past (how manny pennies you have) does not help in calculating the probability of 0.5\n",
    "    - thus, how many pennies you will have after the next toss is independent of how many pennies you had several tosses ago given you currently have three pennies\n",
    "\n",
    "**Definition 5.1**: The stochastic process $X = \\{X_n; n = 0, 1, \\dots\\}$ with discrete state space $E$ is a Markov chain if the following holds for each $j \\in E$ and $n = 0, 1, \\dots$\n",
    "$$\n",
    "\\Pr\\{X_{n+1} = j \\mid X_0 = i_0, X_1 = i_1, \\dots, X_n = i_n\\} = \\Pr\\{X_{n+1} = j \\mid X_n = i_n\\},\n",
    "$$\n",
    "for any set of states $i_0, \\dots, i_n$ in the state space. Furthermore, the Markov chain is said to have stationary transition probabilities if:\n",
    "$$\n",
    "\\Pr\\{X_1 = j \\mid X_0 = i\\} = \\Pr\\{X_{n+1} = j \\mid X_n = i\\}.\n",
    "$$\n",
    "\n",
    "- left-hand-side is the probability of going to state $j$ next, given the history of all past states\n",
    "- right-hand-side is the probability of going to state $j$ next, given only the present state\n",
    "- the stationary property simply indicates that the probability of a one-step transition does not change as time increases (ex: the probabilities are the same in the winter and the summer)\n",
    "- if probabilities are stationary (assumed in this chapter) the only information needed to describe the process are the initial conditions (a probability mass function for $X_0$) and and the one-step transition probabilities\n",
    "\n",
    "A square non-negative matrix is used for the transition probabilities and is often denoted by the capital letter $\\textbf{P}$ where\n",
    "$$\n",
    "P(i, j) = \\Pr\\{X_1 = j \\mid X_0 = i\\}\n",
    "$$\n",
    "\n",
    "- sum of the elements within each row equals one\n",
    "- any nonnegative matrix with row sums equal to one is called a *Markov Matrix*\n",
    "\n",
    "**Example 5.1:** Consider a farmer using an old tractor. The tractor is often in the repair shop but it always takes only one day to get it running again. The first day out of the shop it always works but on any given day thereafter, independent of its previous history, there is a 10% chance of it not working and thus being sent back to the shop. Let $X_0, X_1, \\dots$ be random variables denoting the daily condition of the tractor, where a one denotes the working condition and a zero denotes the failed condition. In other words, $X_n = 1$ denotes that the tractor is working on day $n$ and $X_n = 0$ denotes it being in the repair shop on day $n$. Thus $X_0, X_1, \\dots$ is a Markov chain with state space $E = \\{0, 1 \\}$ and with Markov matrix:\n",
    "\n",
    "$P_1 = \\begin{bmatrix}\n",
    "0 & 1 \\\\\n",
    "0.1 & 0.9 \\\\ \n",
    "\\end{bmatrix}$ \n",
    "\n",
    "- row 1 is state 0\n",
    "- row 2 is state 1\n",
    "\n",
    "**Example 5.3:** Let $X = \\{X_n; n = 0, 1, \\dots\\}$ be a Markov chain with state space $E = \\{1, 2, 3, 4\\}$ and transition probabilities given by the matrix $P$:\n",
    "\n",
    "$\n",
    "P = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 0.3 & 0.7 & 0 \\\\\n",
    "0 & 0.5 & 0.5 & 0 \\\\\n",
    "0.2 & 0.1 & 0.7 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "The chain in this example is structurally different than the other two examples in that you might start in State 4, then go to State 3, and never get to State 1; or you might start in State 4 and go to State 1 and stay there forever. The other two examples, however, involved transitions in which it was always possible to eventually reach every state from every other state.\n",
    "\n",
    "<br>\n",
    "\n",
    "After a Markov chain has been formulated, there are many questions that might be of interest:\n",
    "- In what state will the Markov chain be five steps from now?\n",
    "- What percent of time is spent in a given state?\n",
    "- Starting from one state, will the chain ever reach another fixed state?\n",
    "- What is the long run average profit per unit of time?\n",
    "- etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.2 Multistep Transitions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Markov matrix provides direct information about one-step transition probabilities and it can also be used in the calculation of probabilities for transitions involving more than one step\n",
    "\n",
    "**Example 5.2:** A salesman lives in town 'a' and is responsible for towns 'a', 'b', and 'c'. Each week he is required to visit a different town. When he is in his home town, it makes no difference which town he visits next so he flips a coin and if it is heads he goes to 'b' and if tails he goes to 'c'. However, after spending a week away from home, he has a slight preference for going home, so when he is in either towns 'b' or 'c', he flips two coins. If two heads occur, then he goes to the other town; otherwise he goes to 'a'. The successive towns that he visits form a Markov chain with state space $E = \\{a, b, c\\}$ where the random variable $X_n$ equals $a$, $b$, or $c$ according to his location during week $n$. The state diagram for this system is given in Fig. 5.2 and the associated Markov matrix is:\n",
    "\n",
    "$P = \\begin{bmatrix}\n",
    "0 & 0.50 & 0.50 \\\\\n",
    "0.75 & 0 & 0.25 \\\\\n",
    "0.75 & 0.25 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "- Salesman starts in town b.\n",
    "- Markov matrix indicates that the probability of being in State a after one step (in one week) is 0.75, but what is the probability that he will be in State a after two steps?\\\n",
    "- to compute the probability, we need to sum over all possible routes:\n",
    "\n",
    "$$\n",
    "\\Pr\\{X_2 = a \\mid X_0 = b\\} = \\Pr\\{X_1 = a \\mid X_0 = b\\} \\times \\Pr\\{X_2 = a \\mid X_1 = a\\} \\\\\n",
    "+ \\Pr\\{X_1 = b \\mid X_0 = b\\} \\times \\Pr\\{X_2 = a \\mid X_1 = b\\} \\\\\n",
    "+ \\Pr\\{X_1 = c \\mid X_0 = b\\} \\times \\Pr\\{X_2 = a \\mid X_1 = c\\} \\\\\n",
    "= P(b,a)P(a,a) + P(b,b)P(b,a) + P(b,c)P(c,a).\n",
    "$$\n",
    "\n",
    "- this is definition of matrix multiplication\n",
    "\n",
    "![Img](images/feldman_flores_fig5.4.jpg)\n",
    "\n",
    "\n",
    "**Property 5.1:** Let $X = \\{X_n; n = 0, 1, \\dots\\}$ be a Markov chain with state space $E$ and Markov matrix $P$, then for $i, j \\in E$ and $n = 1, 2, \\dots$\n",
    "$$\n",
    "\\Pr\\{X_n = j \\mid X_0 = i\\} = P^n(i, j),\n",
    "$$\n",
    "where the right-hand side represents the $i-j$ element of the matrix $P^n$.\n",
    "\n",
    "- $P^n(i, j)$ is interpreted to be the probability of going from state $i$ to state $j$ in $n$ steps\n",
    "- Example 5.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.75  , 0.125 , 0.125 ],\n",
       "       [0.1875, 0.4375, 0.375 ],\n",
       "       [0.1875, 0.375 , 0.4375]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example5_2_P_matrix = np.array([[0, 0.5, 0.5], [0.75, 0, 0.25], [0.75, 0.25, 0]])\n",
    "example5_2_P_matrix_sqaured = np.matmul(example5_2_P_matrix, example5_2_P_matrix)\n",
    "example5_2_P_matrix_sqaured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the probability that he will be in State a after two steps given he starts in state b? $\\implies$ $\\textbf{P}^2[1][0] = 0.1875$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov chains are often used to analyze the cost or profit of an operation and thus we need to consider a cost or profit function imposed on the process. \n",
    "- Suppose in Example 5.2 that every week spent in town a resulted in a profit of $1000, every week spent in town b resulted in a profit of $1200, and every week spent in town c resulted in a profit of $1250\n",
    "- What would be the expected profit after the first week if the initial town was town a?\\\n",
    "- More generally, what would be the expected profit of the $n^{th}$ week if the initial town was a? \n",
    "\n",
    "$$\n",
    "E[\\text{Profit for week } n] = P^n(a,a) \\times 1000 + P^n(a,b) \\times 1200 + P^n(a,c) \\times 1250\n",
    "$$\n",
    "\n",
    "**Property 5.2:** Let $X = \\{X_n; n = 0, 1, \\dots\\}$ be a Markov chain with state space $E$, Markov matrix $P$, and profit function $f$ (i.e., each time the chain visits state $i$, a profit of $f(i)$ is obtained). The expected profit at the nth step is given by:\n",
    "$$\n",
    "E[f(X_n) \\mid X_0 = i] = P^n f(i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the expected profit during the second week given that the initial town was a?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1056.25"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profits = np.array([1000, 1200, 1250])\n",
    "ex_profit_vector = np.matmul(example5_2_P_matrix_sqaured, profits)\n",
    "ex_profit_vector[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial state is not always known. Say for example, the manager of the traveling salesman might not know for sure the location of the salesman; instead, all that is known is a probability mass function describing his initial location. Using Example 5.2, suppose that we do not know for sure the salesman’s initial location but know that there is a 50% chance he is in town a, a 30% chance he is in town b, and a 20% chance he is in town c.\n",
    "- What is the probability that the salesman will be in town a next week?\n",
    "\n",
    "$$\n",
    "\\Pr\\{X_1 = a\\} = 0.50 \\times 0 + 0.30 \\times 0.75 + 0.20 \\times 0.75 = 0.375.\n",
    "$$\n",
    "\n",
    "**Property 5.3:** Let $X = \\{X_n; n = 0, 1, \\dots\\}$ be a Markov chain with state space $E$, Markov matrix $P$, and initial probability vector $\\mu$ (i.e., $\\mu(i) = \\Pr\\{X_0 = i\\}$). Then the probability of being in state $j$ at step $n$ starting from the initial distribution is given by:\n",
    "$$\n",
    "\\Pr_\\mu\\{X_n = j\\} = \\mu P^n(j).\n",
    "$$\n",
    "\n",
    "- subscript $\\mu$ is to insure that there is no confusion over the given conditions\n",
    "- last two properties can be combined, when necessary, into one statement\n",
    "\n",
    "**Property 5.4:**\n",
    "Let $X = \\{X_n; n = 0, 1, \\dots\\}$ be a Markov chain with state space $E$, Markov matrix $P$, initial probability vector $\\mu$, and profit function $f$. The expected profit at the nth step is given by:\n",
    "$$\n",
    "E_\\mu[f(X_n)] = \\mu P^n f\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the expected profit in the second week (example 5.2)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1119.375"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_prob_vector = np.array([0.50, 0.30, 0.20])\n",
    "ex_profit_given_inits = np.matmul(init_prob_vector, ex_profit_vector)\n",
    "ex_profit_given_inits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 5.5:** A market analysis concerning consumer behavior in auto purchases has been conducted. Body styles traded-in and purchased have been recorded by a particular dealer with the following results:\n",
    "\n",
    "| Trade-In       | Purchased       | Number of Customers |\n",
    "|----------------|-----------------|---------------------|\n",
    "| Sedan          | Sedan           | 275                 |\n",
    "| Sedan          | Station Wagon   | 180                 |\n",
    "| Sedan          | Convertible     | 45                  |\n",
    "| Station Wagon  | Sedan           | 80                  |\n",
    "| Station Wagon  | Station Wagon   | 120                 |\n",
    "| Convertible    | Sedan           | 150                 |\n",
    "| Convertible    | Convertible     | 50                  |\n",
    "\n",
    "Define the Markov chain to be the body-style of the automobile that the customer has immediately after a trade-in, where the state space is $E = \\{s,w,c\\}$ with s for the sedan, w for the wagon, and c for the convertible. Of 500 customers who have a sedan, 275 will stay with the sedan during the next trade; therefore, the s-s element of the transition probability matrix will be 275/500. Thus, the Markov matrix for the “body style” Markov chain \n",
    "\n",
    "$$\n",
    "P = \\begin{bmatrix}\n",
    "275/500 & 180/500 & 45/500 \\\\\n",
    "80/200 & 120/200 & 0 \\\\\n",
    "150/200 & 0 & 50/200 \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.55 & 0.36 & 0.09 \\\\\n",
    "0.40 & 0.60 & 0 \\\\\n",
    "0.75 & 0 & 0.25 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let us assume we have a customer whose behavior is described by this model. Furthermore, the customer always buys a new car in January of every year. It is now January 1997, and the customer enters the dealership with a sedan ($X_{1996} = s$). The probability that the customer will leave with another sedan is 55%, $\\Pr\\{X_{1997} = s \\mid X_{1996} = s \\}$. Predict what body-style the customer will have for trade-in during January 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5023 , 0.43344, 0.06426],\n",
       "       [0.4816 , 0.468  , 0.0504 ],\n",
       "       [0.5355 , 0.378  , 0.0865 ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example5_5_P_matrix = np.array([[0.55, 0.36, 0.09], [0.40, 0.60, 0], [0.75, 0, 0.25]])\n",
    "example5_5_P_matrix_cubed = np.matmul(example5_5_P_matrix, np.matmul(example5_5_P_matrix, example5_5_P_matrix)) \n",
    "example5_5_P_matrix_cubed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\approx$ 50% chance the customer will enter in the year 2000 with a sedan\n",
    "- $\\approx$ 43% chance with a station wagon\n",
    "- $\\approx$ 7% chance of having a convertible\n",
    "- probability that the customer will leave this year (1997) with a convertible and then leave next year (1998) with a sedan $ = P(s,c) \\times P(c,s) = 0.09 \\times 0.75 = 0.0675$\n",
    "\n",
    "What is the probability that the customer who enters the dealership now (1997) with a sedan will leave with a sedan and also leave with a sedan in the year 2000?\n",
    "$$ \\Pr\\{X_{1997} = s, X_{2000} = s \\mid X_{1996} = s\\} = P(s,s) \\times P^3(s,s) \\approx 0.28. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2762650000000001"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example5_5_P_matrix[0][0] * example5_5_P_matrix_cubed[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that a sedan yields a profit of $1200, a station wagon yields $1500, and a convertible yields $2500, What is expected profit this year from the customer?\n",
    "\n",
    "$E[f(X_{1999}) \\mid X_{1996} = s] \\approx 1414,$ where $f = (1200, 1500, 2500)^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1413.5700000000002"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profit_vector = [1200, 1500, 2500]\n",
    "np.matmul(example5_5_P_matrix_cubed, profit_vector)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.3 Classification of States**\n",
    "\n",
    "- Two types of states possible in a Markov chain\n",
    "- Individual states for a particular chain must be classified into one of these two types\n",
    "- Two random variables that will be extremely important denote “first passage times” and “number of visits to a fixed state”\n",
    "\n",
    "<br>\n",
    "\n",
    "- Consider a fixed state, State $j$ in the state space for a Markov chain. \n",
    "- **First passage time is a random variable**, $T^j$ that equals the time (or number of steps) it takes to reach the fixed state for the first time\n",
    "- Mathematically: the first passage time random variable is defined by:\n",
    "\n",
    "$$\n",
    "T^j = \\min\\{n \\geq 1 : X_n = j\\},\n",
    "$$\n",
    "where the minimum of the empty set is taken to be $+\\infty$\n",
    "\n",
    "(Example 5.3): \\\n",
    "$X = \\{X_n; n = 0, 1, \\dots\\}$ is a Markov chain with state space $E = \\{1, 2, 3, 4\\}$, \n",
    "$\n",
    "P = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 0.3 & 0.7 & 0 \\\\\n",
    "0 & 0.5 & 0.5 & 0 \\\\\n",
    "0.2 & 0.1 & 0.7 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$ \n",
    "\n",
    "- in this markov chain, if $X_0 = 1$ then $T^2 = \\infty$ $\\implies$ if the chain starts in State 1 then it will never reach State 2\n",
    "\n",
    "<br>\n",
    "\n",
    "- **The number of visits to a state is a random variable**, $N^j$\n",
    "- equals the total number of visits (including time zero) that the Markov chain makes to the fixed state throughout the life of the chain\n",
    "- Mathematically, the “number of visits” random variable is defined by:\n",
    "\n",
    "$$\n",
    "N_j = \\sum_{n=0}^\\infty I(X_n, j),\n",
    "$$\n",
    "\n",
    "where $I$ is the identity matrix\n",
    "\n",
    "- identity matrix is used simply as a “counter”\n",
    "- since the the identity has ones on the diagonal and zeroes off the diagonal, $I(X_n, j) = 1$ if $X_n = j$; otherwise it is $0$\n",
    "- if $X_n = j$, then $N^j$ must be at least one because the summation starts at $n = 0$ \n",
    "\n",
    "<br>\n",
    "\n",
    "**Example 5.6:** Let us consider a realization for the Markov chain of Example 5.3. (By realization, we mean conceptually that an experiment is conducted and we record the random outcomes of the chain.) Assume that the first part of the realization (Fig. 5.5) is $X_0 = 4$, $X_1 = 4$, $X_2 = 4$, $X_3 = 3$, $X_4 = 2$, $X_5 = 3$, $X_6 = 2$, \\dots. The first passage random variables for this realization are $T_1 = \\infty$, $T_2 = 4$, $T_3 = 3$, and $T_4 = 1$. To see why $T_1 = \\infty$, it is easiest to refer to the state diagram describing the Markov chain (Fig. 5.3). By inspecting the diagram it becomes obvious that once the chain is in either States 2 or 3, it will never get to State 1 and thus $T_1 = \\infty$.\n",
    "\n",
    "![Img](images/feldman_flores_fig5.5.jpg)\n",
    "\n",
    "- the number of visits random variables are $N_1 = 0, \\quad N_2 = \\infty, \\quad N_3 = \\infty, \\quad N_4 = 3$\n",
    "- Assume that our second realization results in the values $X_0 = 4$, $X_1 = 4$, $X_2 = 1$, $X_3 = 1$, $X_4 = 1$, \\dots.\n",
    "- The new outcomes for the first passage random variables for this second realization are $T_1 = 2$, $T_2 = \\infty$, $T_3 = \\infty$, and $T_4 = 1$.\n",
    "  - $N_1 = \\infty$, $N_2 = 0$, $N_3 = 0$, and $N_4 = 2$.\n",
    "- should be able to describe its probability mass function, not the value of $T^j$ will be before an experiment\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "- Primary quantity of interest regarding the first passage times are the **first passage probabilities**\n",
    "- question of interest is whether or not it is possible to reach a particular state from a given initial state\n",
    "- to answer, determine the first passage probability $F(i, j)$ which is the probability of eventually reaching State $j$ at least once given that the initial state was State $i$\n",
    "\n",
    "$$\n",
    "F(i, j) = \\Pr\\{T^j < \\infty \\mid X_0 = i\\}.\n",
    "$$\n",
    "\n",
    "\n",
    "First passage probabilities for the chain of Example 5.3 are given by:\n",
    "$$\n",
    "P = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 1 & 0 \\\\\n",
    "0 & 1 & 1 & 0 \\\\\n",
    "<1 & <1 & <1 & <1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Primary quantity of interest for the number of visits random variable is its expected value\n",
    "- The expected number of visits to State $j$ given the initial state was $i$ is denoted by $R(i, j)$\n",
    "\n",
    "$$\n",
    "R(i, j) = E[N_j \\mid X_0 = i].\n",
    "$$\n",
    "\n",
    "$$\n",
    "P = \\begin{bmatrix}\n",
    "\\infty & 0 & 0 & 0 \\\\\n",
    "0 & \\infty & \\infty & 0 \\\\\n",
    "0 & \\infty & \\infty & 0 \\\\\n",
    "\\infty &\\infty & \\infty & <\\infty \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "- High level takeaway:\n",
    "  - $\\textbf{F}$ matrix is whether an element is zero or one\n",
    "  - $\\textbf{R}$ matrix is whether an element is zero or infinity\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**Property 5.5:** Let $R(i, j)$ is defined as the expected number of visits to State $j$ given the initial state was $i$ and $F(i, j)$ is defined as the probability of eventually reaching State $j$ at least once given that the initial state was State $i$. Then\n",
    "\n",
    "<style>\n",
    "    .math-large {\n",
    "        font-size: 125%; \n",
    "    }\n",
    "</style>\n",
    "<div class=\"math-large\">\n",
    "\n",
    "$$\n",
    "R(i, j) = \n",
    "\\begin{cases} \n",
    "\\frac{1}{1 - F(j, j)} & \\text{for } i = j, \\\\\n",
    "\\frac{F(i, j)}{1 - F(j, j)} & \\text{for } i \\neq j;\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "</div>\n",
    "\n",
    "where the convention $\\frac{0}{0} = 0$ is used.\n",
    "\n",
    "- in example 5.3\n",
    "  - if the chain ever gets to State 1 or to State 2 or to State 3 then that state will continually reoccur (recurrent)\n",
    "  - if the chain starts in State 4, it will only stay there a finite number of times and will eventually leave State 4 never to return (transient)\n",
    "  - recurrent and transient states\n",
    "\n",
    "**Definition 5.2:** \\\n",
    "A state $j$ is called **transient** if $F(j, j) < 1$. Equivalently, state $j$ is transient if $R(j, j) < \\infty$.\n",
    "\n",
    "**Definition 5.3:** \\\n",
    "A state $j$ is called **recurrent** if $F(j, j) = 1$. Equivalently, state $j$ is recurrent if $R(j, j) = \\infty$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Img](images/feldman_flores_fig5.7.jpg)\n",
    "\n",
    "**Example 5.7:** Let $X = \\{X_n; n = 0, 1, \\dots\\}$ be a Markov chain with state space $E = \\{1, 2, 3, 4\\}$ and transition probabilities given by:\n",
    "$$\n",
    "\n",
    "P = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0.01 & 0.29 & 0.7 & 0 \\\\\n",
    "0 & 0.5 & 0.5 & 0 \\\\\n",
    "0.2 & 0 & 0.1 & 0.7 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- chain of this example has one recurrent state and three transient states\n",
    "\n",
    "$$\n",
    "F = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "1 & <1 & <1 & 0 \\\\\n",
    "1 & 1 & <1 & 0 \\\\\n",
    "1 & <1 & <1 & <1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The matrix \\( R \\) is given by:\n",
    "$$\n",
    "R = \\begin{bmatrix}\n",
    "\\infty & 0 & 0 & 0 \\\\\n",
    "\\infty & <\\infty & <\\infty & 0 \\\\\n",
    "\\infty & <\\infty & <\\infty & 0 \\\\\n",
    "\\infty & <\\infty & <\\infty & <\\infty \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition 5.4:** Let $X = \\{X_n; n = 0, 1, \\dots\\}$ be a Markov chain with Markov matrix $P$ and let $C$ be a set of states contained in its state space. Then $C$ is **closed** if \n",
    "$$\n",
    "\\sum_{j \\in C} P(i, j) = 1 \\text{ for all } i \\in C.\n",
    "$$\n",
    "\n",
    "- To illustrate the concept of closed sets, refer again to Example 5.3. The sets {2,3,4}, {2}, and {3,4} are not closed sets. The set {1,2,3,4} is obviously closed, but it can be reduced to a smaller closed set. The set {1,2,3} is also closed, but again it can be further reduced. Both sets {1} and {2,3} are closed and cannot be reduced further. This idea of taking a closed set and trying to reduce it is extremely important and leads to the definition of an irreducible set.\n",
    "\n",
    "**Definition 5.5:** A closed set of states that contains no proper subset which is also closed is called irreducible. A state that forms an irreducible set by itself is called an absorbing state.\n",
    "\n",
    "- The Markov chain of Example 5.3 has two irreducible sets: the set {1} and the set {2,3}. Since the first irreducible set contains only one state, that state is an absorbing state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Img](images/feldman_flores_fig5.8.jpg)\n",
    "\n",
    "**Example 5.9:** Let $X$ be a Markov chain with state space $E = \\{a, \\dots, g\\}$ and Markov matrix $P$ given by:\n",
    "\n",
    "$$\n",
    "P = \\begin{bmatrix}\n",
    "0.3 & 0.7 & 0   & 0   & 0   & 0   & 0 \\\\\n",
    "0.5 & 0.5 & 0   & 0   & 0   & 0   & 0 \\\\\n",
    "0   & 0.2 & 0.4 & 0.4 & 0   & 0   & 0 \\\\\n",
    "0   & 0   & 0.5 & 0.5 & 0   & 0   & 0 \\\\\n",
    "0   & 0   & 0   & 0.8 & 0.1 & 0.1 & 0 \\\\\n",
    "0   & 0   & 0   & 0   & 0.7 & 0.3 & 0 \\\\\n",
    "0   & 0   & 0   & 0   & 0   & 0.4 & 0.6 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- states a and b are recurrent and all others are transient\n",
    "  - once the chain reaches either a or b, it will stay in those two states, no paths leading away from the set {a,b}\n",
    "  - all states will eventually reach State b\n",
    "- Since no state goes to g, we can exclude g and still have a closed set; {a,b,c,d,e,f} is closed\n",
    "- set {a,b,c,d,e,f} can be reduced to a smaller closed set so it is not irreducible\n",
    "- Because there is a path going from e to f , the set {a,b,c,d,e} is not closed\n",
    "  - excluding the state e, a closed set is again; {a,b,c,d} is closed and is not irreducible\n",
    "- consider the set {a,b}. This two-state set is closed and cannot be further reduced to a smaller closed set; therefore, the set {a,b} is an irreducible set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Property 5.6:** All states within an irreducible set are of the same classification\n",
    "- if you can identify one state within an irreducible set as being transient, then all states within the set are transient\n",
    "- if one state is recurrent, then all states within the set are recurrent.\n",
    "\n",
    "**Property 5.7:** Let C be an irreducible set of states such that the number of states within C is finite. Then each state within C is recurrent.\n",
    "- Communication between states is like communication between people; there is communication only if messages can go both ways\n",
    "- two states, i and j, communicate if and only if it is possible to eventually reach j from i and it is possible to eventually reach i from j\n",
    "- Example 5.3: States 2 and 3 communicate, but States 4 and 2 do not communicate\n",
    "- communication must be both ways but it does not have to be in one step\n",
    "\n",
    "**Property 5.8:**. The closed set of states C is irreducible if and only if every state within C communicates with every other state within C.\n",
    "- The procedure for identifying irreducible sets of states for a Markov chain with a finite state space is to first draw the state diagram, then pick a state and identify all states that communicate with it. If the set made up of all those states that communicate with it is closed, then the set is an irreducible, recurrent set of states. If it is not closed, then the originally chosen state and all the states that communicate with it are transient states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.4 Steady-State Behavior**\n",
    "\n",
    "![Img](images/feldman_flores_fig5.11.jpg)\n",
    "\n",
    "- Probabilities of being in State a at time n given that at time zero the chain was in State a\n",
    "- the graph gives the values of $P^n(a,a)$ as a function of $n$\n",
    "\n",
    "$$\n",
    "\\lim_{n \\to \\infty} \\Pr\\{X_n = a \\mid X_0 = a\\} = 0.42857.\n",
    "$$\n",
    "\n",
    "- if you spent the time to graph the probabilities of being in State a starting from State b instead of State a\n",
    "\n",
    "$$\n",
    "\\lim_{n \\to \\infty} \\Pr\\{X_n = a \\mid X_0 = b\\} = 0.42857.\n",
    "$$\n",
    "\n",
    "- When discussing steady-state (or limiting) conditions, enough time has elapsed so that the probabilities do not change with respect to time\n",
    "  - not that the chain stops changing (dynamic by defintion)\n",
    "- Often, steady-state results are independent of initial conditions $\\implies$ not always true\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Irreducible recurrent Markov chain:** the entire state space of the Markov chain forms an irreducible recurrent set\n",
    "  - steady-state probabilities are independent of the initial state, can use Property 5.9\n",
    "\n",
    "**Property 5.9:** Let $X = \\{X_n; n = 0, 1, \\dots\\}$ be a Markov chain with finite state space $E$ and Markov matrix $P$. Furthermore, assume that the entire state space forms an irreducible, recurrent set, and let\n",
    "$$\n",
    "\\pi(j) = \\lim_{n \\to \\infty} \\Pr\\{X_n = j \\mid X_0 = i\\}.\n",
    "$$\n",
    "The vector $\\pi$ is the solution to the following system:\n",
    "$$\n",
    "\\pi P = \\pi,\n",
    "$$\n",
    "$$\n",
    "\\sum_{i \\in E} \\pi(i) = 1.\n",
    "$$\n",
    "\n",
    "**Example 5.2:** \n",
    "$P = \\begin{bmatrix}\n",
    "0 & 0.50 & 0.50 \\\\\n",
    "0.75 & 0 & 0.25 \\\\\n",
    "0.75 & 0.25 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "- Applying Property 5.9 to Example 5.2:\n",
    "$$\n",
    "0.75\\pi_b + 0.75\\pi_c = \\pi_a,\n",
    "$$\n",
    "$$\n",
    "0.5\\pi_a + 0.25\\pi_c = \\pi_b,\n",
    "$$\n",
    "$$\n",
    "0.5\\pi_a + 0.25\\pi_b = \\pi_c,\n",
    "$$\n",
    "$$\n",
    "\\pi_a + \\pi_b + \\pi_c = 1.\n",
    "$$\n",
    "\n",
    "- four equations and only three variables, normally there would not be a unique solution but for an irreducible Markov matrix there is always exactly one redundant equation from the system formed by: $\\textbf{π} \\textbf{P} = \\textbf{π}$\n",
    "- to solve, arbitrarily choose one of the first three equations to discard and solve the remaining 3 by 3 system\n",
    "  - never discard the final or norming equation\n",
    "\n",
    "$$\n",
    "\\pi_a = \\frac{3}{7}, \\pi_b = \\frac{2}{7},\\pi_c = \\frac{2}{7}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- All irreducible, recurrent sets must be identified and grouped together and then the Markov matrix is rearranged so that the irreducible sets are together and transient states are last.\n",
    "- Markov matrix for a chain can always be rewritten in the form: \n",
    "\n",
    "$$\n",
    "P = \\begin{bmatrix}\n",
    "P_1 &  &  & \\cdots  \\\\\n",
    " & P_2 &  & \\cdots  \\\\\n",
    " &  & P_3 & \\cdots \\\\\n",
    " &  & \\vdots & \\ddots & \\\\\n",
    "B_1 & B_2 & B_3 & \\cdots & Q \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- After a chain is in this form, each submatrix $\\textbf{P}_i$ is a Markov matrix and can be considered as an independent Markov chain for which Property 5.9 is applied.\n",
    "\n",
    "\n",
    "**Example 5.3:** state space $E = \\{1, 2, 3, 4\\}$, $\n",
    "P = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 0.3 & 0.7 & 0 \\\\\n",
    "0 & 0.5 & 0.5 & 0 \\\\\n",
    "0.2 & 0.1 & 0.7 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "- State 1 is absorbing (i.e., an irreducible set of one state)\n",
    "  - steady-state probability is $\\pi_1 = 1$\n",
    "- States 2 and 3 form an irreducible set so Property 5.9 can be applied to the submatrix\n",
    "\n",
    "$$\n",
    "0.3\\pi_2 + 0.5\\pi_3 = \\pi_2,\n",
    "$$\n",
    "$$\n",
    "0.7\\pi_2 + 0.5\\pi_3 = \\pi_3,\n",
    "$$\n",
    "$$\n",
    "\\pi_2 + \\pi_3 = 1.\n",
    "$$\n",
    "\n",
    "yields,\n",
    "\n",
    "$$\n",
    "\\pi_2 = \\frac{5}{12}, \\pi_3 = \\frac{7}{12}\n",
    "$$\n",
    "\n",
    "- The values π2 and π3 are interpreted to mean that if a snapshot of the chain is taken a long time after it started and if it started in States 2 or 3, then there is a 5/12 probability that the picture will show the chain in State 2 and a 7/12 probability that it will be in State 3. \n",
    "- In other words, over the long run the fraction of time spent in State 2 would equal 5/12 and the fraction of time spent in State 3 would equal 7/12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Property 5.10 (Ergodic Property):** Let $X = \\{X_n; n = 0, 1, \\dots\\}$ be a Markov chain with a finite state space containing $k$ distinct irreducible sets. Let the $\\mathcal{l}^{th}$ irreducible set be denoted by $C_\\mathcal{l}$, and let $P_\\mathcal{l}$ be the Markov matrix restricted to the $\\mathcal{l}^{th}$ irreducible set (as in the form where Markov matrix is rearranged so that the irreducible sets are together and transient states are last). Finally, let $F$ be the matrix of first passage probabilities.\n",
    "\n",
    "- **If State $j$ is transient:**\n",
    "  $$\n",
    "  \\lim_{n \\to \\infty} \\Pr\\{X_n = j \\mid X_0 = i\\} = 0.\n",
    "  $$\n",
    "\n",
    "- **If State $i$ and $j$ both belong to the $\\mathcal{l}^{th}$ irreducible set:**\n",
    "  $$\n",
    "  \\lim_{n \\to \\infty} \\Pr\\{X_n = j \\mid X_0 = i\\} = \\pi(j),\n",
    "  $$\n",
    "  where\n",
    "  $$\n",
    "  \\pi P_\\mathcal{l} = \\pi \\quad \\text{and} \\quad \\sum_{i \\in C_\\mathcal{l}} \\pi(i) = 1.\n",
    "  $$\n",
    "\n",
    "- **If State $j$ is recurrent and $i$ is not in its irreducible set:**\n",
    "  $$\n",
    "  \\lim_{n \\to \\infty} \\Pr\\{X_n = j \\mid X_0 = i\\} = F(i, j) \\pi(j),\n",
    "  $$\n",
    "  where $\\pi$ is determined as above.\n",
    "\n",
    "- **If State $j$ is recurrent and $X_0$ is in the same irreducible set as $j$:**\n",
    "  $$\n",
    "  \\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{m=0}^{n-1} I(X_m, j) = \\pi(j),\n",
    "  $$\n",
    "  where $I$ is the identity function.\n",
    "\n",
    "- **If State $j$ is recurrent:**\n",
    "  $$\n",
    "  E[T_j \\mid X_0 = j] = \\frac{1}{\\pi(j)}.\n",
    "  $$\n",
    "\n",
    "- identity matrix acts as a counter so that the summation on the left-hand-side of the equation is the total number of visits that the chain makes to state j\n",
    "- equality indicates that the fraction of time spent in State j is equal to the steady-state probability of being in State j\n",
    "- the reciprocal of the long-run probabilities equals the expected number of steps to return to the state $\\implies$ higher the probability, the quicker the return\n",
    "\n",
    "<br>\n",
    "\n",
    "![Img](images/feldman_flores_ex5.11.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 5.12:** The market analysis discussed in Example 5.5 gave switching probabilities for body styles and associated profits with each body style. The long behavior for a customer can be estimated by calculating the long-run probabilities. \n",
    "\n",
    "| Trade-In       | Purchased       | Number of Customers |\n",
    "|----------------|-----------------|---------------------|\n",
    "| Sedan          | Sedan           | 275                 |\n",
    "| Sedan          | Station Wagon   | 180                 |\n",
    "| Sedan          | Convertible     | 45                  |\n",
    "| Station Wagon  | Sedan           | 80                  |\n",
    "| Station Wagon  | Station Wagon   | 120                 |\n",
    "| Convertible    | Sedan           | 150                 |\n",
    "| Convertible    | Convertible     | 50                  |\n",
    "\n",
    "$$\n",
    "P = \\begin{bmatrix}\n",
    "275/500 & 180/500 & 45/500 \\\\\n",
    "80/200 & 120/200 & 0 \\\\\n",
    "150/200 & 0 & 50/200 \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.55 & 0.36 & 0.09 \\\\\n",
    "0.40 & 0.60 & 0 \\\\\n",
    "0.75 & 0 & 0.25 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "0.36\\pi_s + 0.60\\pi_w = \\pi_w,\n",
    "$$\n",
    "$$\n",
    "0.09\\pi_s + 0.25\\pi_c = \\pi_c,\n",
    "$$\n",
    "$$\n",
    "\\pi_s + \\pi_w + \\pi_c = 1.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\implies\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\pi = (0.495,0.446,0.059)\n",
    "$$\n",
    "$$\n",
    "\\text{the long-run expected profit per customer trade-in} = 0.495×1200+0.446×1500+0.059×2500 = 1410.5\n",
    "$$\n",
    "\n",
    "(stationary probabilities dot profit vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5.5 Computations**\n",
    "\n",
    "The determination of $R$ (Eq. 5.5) is straightforward for recurrent states. If States $i$ and $j$ are in the same irreducible set, then $R(i, j) = \\infty$. If $i$ is recurrent and $j$ is either transient or in a different irreducible set than $i$, then $R(i, j) = F(i, j) = 0$. For $i$ transient and $j$ recurrent, then $R(i, j) = \\infty$ if $F(i, j) > 0$ and $R(i, j) = 0$ if $F(i, j) = 0$. In the case where $i$ and $j$ are transient, additional work is required as given in the following property:\n",
    "\n",
    "**Property 5.11:** \\\n",
    "Let $X = \\{X_n; n = 0, 1, \\dots\\}$ be a Markov chain and let $A$ denote the (finite) set of all transient states. Let $Q$ be the matrix of transition probabilities restricted to the set $A$, then, for $i, j \\in A$:\n",
    "\n",
    "$$\n",
    "R(i, j) = (I - Q)^{-1}(i, j)\n",
    "$$\n",
    "\n",
    "- This is expected number of visits to State $j$ given the initial state was $i$ \n",
    "\n",
    "<br>\n",
    "\n",
    "The matrix P must be rewritten so that each irreducible, recurrent set is treated as a single “super” state. Once the Markov chain gets into an irreducible, recurrent set, it will remain in that set forever and all states within the set will be visited infinitely often. Therefore, in determining the probability of reaching a recurrent state from a transient state, it is only necessary to find the probability of reaching the appropriate irreducible set. The transition matrix in which each irreducible set is treated as a single state is denoted by $\\hat{P}$. The matrix $\\hat{P}$ has the form:\n",
    "\n",
    "$$\n",
    "\\hat{P} = \\begin{bmatrix}\n",
    "1 &  &  & \\cdots  \\\\\n",
    " & 1 &  & \\cdots  \\\\\n",
    " &  & 1 & \\cdots \\\\\n",
    " &  & \\vdots & \\ddots & \\\\\n",
    "b_1 & b_2 & b_3 & \\cdots & Q \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $b_l$ is a vector giving the one-step probability of going from transient state i to irreducible set:\n",
    "\n",
    "$$\n",
    "b_l(i) = \\sum_{j \\in C} P(i, j)\n",
    "$$\n",
    "\n",
    "for $i$ transient and $C_l$ denoting the $l^{th}$ irreducible set\n",
    "\n",
    "**Property 5.12:** \\\n",
    "Let $\\{X_n; n = 0, 1, \\dots\\}$ be a Markov chain with a finite state space ordered so that its Markov matrix can be reduced to the form in Eq. (5.9). Then, for a transient state $i$ and a recurrent state $j$, we have:\n",
    "$$\n",
    "F(i, j) = ((I-Q)^{-1}b)(i)\n",
    "$$\n",
    "for each state $j$ in the $l^{th}$ irreducible set.\n",
    "\n",
    "- First passage probability, probability of eventually reaching State $j$ at least once given that the initial state was State $i$\n",
    "\n",
    "**Property 5.13:** \\\n",
    "Let $X = \\{X_n; n = 0, 1, \\dots\\}$ be an irreducible Markov chain with finite state space $E$ and with steady-state probabilities given by the vector $\\pi$. Let the vector $f$ be a profit function (i.e., $f(i)$ is the profit received for each visit to State $i$). Then (with probability one) the long-run average profit per unit of time is:\n",
    "$$\n",
    "\\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{k=0}^{n-1} f(X_k) = \\sum_{j \\in E} \\pi(j)f(j).\n",
    "$$\n",
    "\n",
    "**Property 5.14:** \\\n",
    "Let $X = \\{X_n; n = 0, 1, \\dots\\}$ be a Markov chain with Markov matrix $P$. Let $f$ be a return function and let $\\alpha$ (where $0 < \\alpha < 1$) be a discount factor. Then the expected total discounted cost is given by:\n",
    "$$\n",
    "E\\left[\\sum_{n=0}^{\\infty} \\alpha^n f(X_n) \\mid X_0 = i\\right] = ((I-\\alpha P)^{-1}f)(i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"math-large\">\n",
    "\n",
    "$$\n",
    "R(i, j) = \n",
    "\\begin{cases} \n",
    "\\frac{1}{1 - F(j, j)} & \\text{for } i = j, \\\\\n",
    "\\frac{F(i, j)}{1 - F(j, j)} & \\text{for } i \\neq j;\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"math-large\">\n",
    "\n",
    "$$\n",
    "F(i, j) = \n",
    "\\begin{cases} \n",
    "1 - \\frac{1}{R(j, j)} & \\text{for } i = j, \\\\\n",
    "\\frac{R(i, j)}{R(j, j)} & \\text{for } i \\neq j;\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 5.14:** A missile is launched and, as it is tracked, a sequence of course correction signals are sent to it. Suppose that the system has four states that are labeled as follows. \n",
    "- State 0: on-course, no further correction necessary\n",
    "- State 1: minor deviation\n",
    "- State 2: major deviation\n",
    "- State 3: abort, off-course so badly a self-destruct signal is sent\n",
    "\n",
    "Let $X_n$ represent the state of the system after the $n^th$ course correction and assume that the behavior of $X$ can be modeled by a Markov chain with the following probability transition matrix:\n",
    "\n",
    "$$\n",
    "P = \\begin{bmatrix}\n",
    "1.0 & 0.0 & 0.0 & 0.0 \\\\\n",
    "0.5 & 0.25 & 0.25 & 0.0 \\\\\n",
    "0.0 & 0.5 & 0.25 & 0.25 \\\\\n",
    "0.0 & 0.0 & 0.0 & 1.0 \\\\\n",
    "\\end{bmatrix},\n",
    "\\;\n",
    "\\text{State space } E = \\{1,2,3,4\\} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q = \\begin{bmatrix}\n",
    "0.25 & 0.25 \\\\\n",
    "0.5 & 0.25 \\\\\n",
    "\\end{bmatrix}, \\; E' \\subset E, \\; E' = \\{2,3\\} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.71428571, 0.57142857],\n",
       "       [1.14285714, 1.71428571]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.array([[0.25, 0.25], [0.5, 0.25]])\n",
    "R = np.linalg.inv(np.identity(2) - Q)\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85714286, 0.57142857])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.14285714, 0.42857143])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bs = np.array([[0.5, 0], [0, 0], [0, 0], [0, 0.25]])\n",
    "for b in bs:\n",
    "    display(np.matmul(R, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4166666652083333\n",
      "0.33333333333333337\n",
      "0.6666666666666667\n",
      "0.4166666652083333\n"
     ]
    }
   ],
   "source": [
    "R = [[0, 0 ,0 ,0], [0, 1.71428571, 0.57142857, 0], [0, 1.14285714, 1.71428571, 0], [0, 0, 0,0 ]]\n",
    "F_11 = 1 - 1/R[1][1]\n",
    "F_12 = R[1][2] / R[2][2]\n",
    "F_21 = R[2][1] / R[1][1] \n",
    "F_22 = 1 - 1/R[1][1]\n",
    "\n",
    "print(F_11) \n",
    "print(F_12) \n",
    "print(F_21) \n",
    "print(F_22) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that upon launch, the missile starts in State 2. The probability that it will eventually get on-course is 57.1% (namely, F(2,0)); whereas, the probability that it will eventually have to be destroyed is 42.9%. When a missile is launched, 50,000 pounds of fuel are used. Every time a minor correction is made, 1,000 pounds of fuel are used; and every time a major correction is made, 5,000 pounds of fuel are used. Assume that the missile started in State 2, we wish to determine the expected fuel usage for the mission. This calculation is\n",
    "\n",
    "$$\n",
    "50000 + 1000 \\times R(2,1) + 5000 \\times R(2,2) = 59713\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 5 - Markov Chains - Ross**\n",
    "\n",
    "## **4.1 Introduction**\n",
    "\n",
    "Let $\\{X_n, n = 0, 1, 2, \\dots\\}$ be a stochastic process that takes on a finite or countable number of possible values. Unless otherwise mentioned, this set of possible values of the process will be denoted by the set of nonnegative integers $\\{0, 1, 2, \\dots\\}$. If $X_n = i$, then the process is said to be in state $i$ at time $n$. We suppose that whenever the process is in state $i$, there is a fixed probability $P_{ij}$ that it will next be in state $j$. That is, we suppose that:\n",
    "$$\n",
    "\\Pr\\{X_{n+1} = j \\mid X_n = i, X_{n-1} = i_{n-1}, \\dots, X_1 = i_1, X_0 = i_0\\} = P_{ij}\n",
    "$$\n",
    "\n",
    "For all states $i_0, i_1, \\dots, i_{n-1}, i, j$ and all $n \\geq 0$, such a stochastic process is known as a **Markov chain**. \n",
    "\n",
    "- For a Markov chain, the conditional distribution of any future state $X_{n+1}$, given the past states $X_0, X_1, \\dots, X_{n-1}$ and the present state $X_n$, is independent of the past states and depends only on the present state\n",
    "- The value $P_{ij}$ represents the probability that the process will, when in state $i$, next make a transition into state $j$\n",
    "- The transition probabilities $P_{ij}$ satisfy the following conditions:\n",
    "$$\n",
    "P_{ij} \\geq 0, \\quad \\text{for all } i, j \\geq 0;\n",
    "$$\n",
    "$$\n",
    "\\sum_{j=0}^{\\infty} P_{ij} = 1, \\quad \\text{for } i = 0, 1, \\dots\n",
    "$$\n",
    "\n",
    "Let $\\textbf{P}$ denote the matrix of one-step transition probabilities $P_{ij}$, so that\n",
    "$$\n",
    "\\textbf{P} = \\begin{bmatrix}\n",
    "P_{00} & P_{01} & P_{02} & \\cdots \\\\\n",
    "P_{10} & P_{11} & P_{12} & \\cdots \\\\\n",
    "\\vdots & \\vdots & \\vdots &        \\\\\n",
    "P_{i0} & P_{i1} & P_{i2} & \\cdots \\\\\n",
    "\\vdots & \\vdots & \\vdots &        \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4.1:** (Forecasting the Weather). Suppose that the chance of rain tomorrow depends on previous weather conditions only through whether or not it is raining today and not on past weather conditions. Suppose also that if it rains today, then it will rain tomorrow with probability $\\alpha$ and if it does not rain today, then it will rain tomorrow with probability $\\beta$.\n",
    "\n",
    "$$\n",
    "\\textbf{P} = \\begin{bmatrix}\n",
    "\\alpha & 1 - \\alpha \\\\\n",
    "\\beta & 1 - \\beta \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4.2:** (A Communications System). Consider a communications system that transmits the digits 0 and 1. Each digit transmitted must pass through several stages, at each of which there is a probabilitY $p$ that the digit entered will be unchanged when it leaves. Letting $X_n$ denote the digit entering the $n$th stage, then $\\{X_n, n = 0, 1, \\dots \\}$ is a two-state Markov chain having a transition probability matrix:\n",
    "\n",
    "$$\n",
    "\\textbf{P} = \\begin{bmatrix}\n",
    "p & 1 - p \\\\\n",
    "1 - p & p \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4.3:** On any given day Gary is either cheerful (C), so-so (S), or glum (G). If he is cheerful today, then he will be C,S, or G tomorrow with respective probabilities 0.5, 0.4, 0.1. If he is feeling so-so today, then he will be $C,S, or G$ tomorrow with probabilities 0.3, 0.4, 0.3. If he is glum today, then he will be C,S, or G tomorrow with probabilities 0.2, 0.3, 0.5. Letting $X_n$ denote Gary’s mood on the $n$th day, then $\\{X_n, n \\geq 0\\}$ is a three-state Markov chain (state 0 = $C$, state 1 = $S$, state 2 = $G$) with transition probability matrix\n",
    "\n",
    "$$\n",
    "\\textbf{P} = \\begin{bmatrix}\n",
    "0.5 & 0.4 & 0.1 \\\\\n",
    "0.3 & 0.4 & 0.3 \\\\\n",
    "0.2 & 0.3 & 0.5 \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4.4:** (Transforming a Process into a Markov Chain). Suppose that whether or not it rains today depends on previous weather conditions through the last two days. Specifically, suppose that if it has rained for the past two days, then it will rain tomorrow with probability 0.7; if it rained today but not yesterday, then it will rain tomorrow with probability 0.5; if it rained yesterday but not today, then it will rain tomorrow with probability 0.4; if it has not rained in the past two days, then it will rain tomorrow with probability 0.2.\n",
    "\n",
    "- state 0 $\\implies$ if it rained both today and yesterday\n",
    "- state 1 $\\implies$ if it rained today but not yesterday\n",
    "- state 2 $\\implies$ if it rained yesterday but not today\n",
    "- state 3 $\\implies$ if it did not rain either yesterday or today\n",
    "\n",
    "$$\n",
    "\\textbf{P} = \\begin{bmatrix}\n",
    "0.7 & 0 & 0.3 & 0 \\\\\n",
    "0.5 & 0 & 0.5 & 0 \\\\\n",
    "0 & 0.4 & 0 & 0.6 \\\\\n",
    "0 & 0.2 & 0 & 0.8 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- row is today, col is next day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4.5:** (A Random Walk Model). A Markov chain whose state space is given by the integers $i = 0, \\pm 1, \\pm 2, \\dots$ is said to be a random walk if, for some number $0 < p < 1$, \\\n",
    "\n",
    "$$\n",
    "P_{i,i+1} = p = 1 - P_{i,i-1}, \\;\\; i = 0, \\pm 1, \\dots\n",
    "$$\n",
    "\n",
    "- think of it as being a model for an individual walking on a straight line who at each point of time either takes one step to the right with probability $p$ or one step to the left with probability $1 − p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4.6:** (A Gambling Model). Consider a gambler who, at each play of the game, either wins 1 dollar with probability $p$ or loses 1 dollar with probability $1 − p$. If we suppose that our gambler quits playing either when he goes broke or he attains a fortune of $N$ dollars, then the gambler’s fortune is a Markov chain having transition probabilities\n",
    "\n",
    "$$\n",
    "P_{i,i+1} = p = 1 - P_{i,i-1}, \\;\\; i = 1, 2, \\dots, N - 1 \\\n",
    "$$\n",
    "$$\n",
    "P_{00} = P_{NN} = 1\n",
    "$$\n",
    "\n",
    "- States 0 and N are called absorbing states since once entered they are never left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.2 Chapman–Kolmogorov Equations**\n",
    "\n",
    "- Method to compute $n$-step transition probabilities:\n",
    "$$\n",
    "P_{n+m}^{ij} = \\sum_{k=0}^{\\infty} P_n^{ik} P_m^{kj} \\quad \\text{for all } n, m \\geq 0, \\text{ all } i, j\n",
    "$$\n",
    "\n",
    "- $P_n^{ik} P_m^{kj}$ represents the probability that starting in $i$ the process will go to state $j$ in $n + m$ transitions through a path which takes it into state $k$ at the nth transition\n",
    "\n",
    "\n",
    "Let $\\textbf{P}^{(n)}$ denote the matrix of n-step transition probabilities $P_{n}^{ij}$, from first Chapman–Kolmogorov equation $\\implies$\n",
    "\n",
    "$$\n",
    "\\textbf{P}^{(n + m)} = \\textbf{P}^{(n)} + \\textbf{P}^{(m)} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4.8:** Consider Example 4.1 in which the weather is considered as a two-state Markov chain. If $α$ = 0.7 and $β$ = 0.4, then calculate the probability that it will rain four days from today given that it is raining today.\n",
    "\n",
    "$$\n",
    "\\textbf{P} = \\begin{bmatrix}\n",
    "0.7 & 0.3 \\\\\n",
    "0.4 & 0.6 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{P}^4 = \n",
    "$$\n",
    "\n",
    "$$\n",
    "P_{00}^4 = \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5749, 0.4251],\n",
       "       [0.5668, 0.4332]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.array([[0.7, 0.3], [0.4, 0.6]])\n",
    "p_squared = np.matmul(p, p)\n",
    "p_quad = np.matmul(p_squared, p_squared)\n",
    "p_quad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4.9:** Consider Example 4.4. Given that it rained on Monday and Tuesday, what is the probability that it will rain on Thursday?\n",
    "\n",
    "$$\n",
    "\\textbf{P} = \\begin{bmatrix}\n",
    "0.7 & 0 & 0.3 & 0 \\\\\n",
    "0.5 & 0 & 0.5 & 0 \\\\\n",
    "0 & 0.4 & 0 & 0.6 \\\\\n",
    "0 & 0.2 & 0 & 0.8 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{P}^2 =\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Probability that it will rain on Thursday} = \\text{in state 0 or state 1 on Thursday} = P_{00}^2 + P_{01}^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49, 0.12, 0.21, 0.18],\n",
       "       [0.35, 0.2 , 0.15, 0.3 ],\n",
       "       [0.2 , 0.12, 0.2 , 0.48],\n",
       "       [0.1 , 0.16, 0.1 , 0.64]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6099999999999999"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.array([[0.7, 0, 0.3, 0], [0.5, 0, 0.5, 0], [0, 0.4, 0, 0.6], [0, 0.2, 0, 0.8]])\n",
    "p_squared = np.matmul(p, p)\n",
    "display(p_squared)\n",
    "p_squared[0][0] + p_squared[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4.10:** An urn always contains 2 balls. Ball colors are red and blue. At each stage a ball is randomly chosen and then replaced by a new ball, which with probability 0.8 is the same color, and with probability 0.2 is the opposite color, as the ball it replaces. If initially both balls are red, find the probability that the fifth ball selected is red. \n",
    "\n",
    "- the probability that a selection is red is determined by the composition of the urn at the time of the selection\n",
    "- $X_n$ is the number of red balls in the urn after the nth selection and subsequent replacement\n",
    "- $X_n, n /geq 0$ is a markov chain with states 0,1,2 \n",
    "\n",
    "$$\n",
    "\\textbf{P} = \\begin{bmatrix}\n",
    "0.8 & 0.2 & 0 \\\\ \n",
    "0.1 & 0.8 & 0.1 \\\\ \n",
    "0 & 0.2 & 0.8 \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- to go from 1 red ball in the urn to 0 red balls, the ball chosen must be red (prob = 0.5) and it must then be replaced by a ball of opposite color (prob = 0.2)\n",
    "  - $P_{1,0} = (0.5) * (0.2) = 0.1$\n",
    "\n",
    "- the probability that the fifth selection is red:\n",
    "\n",
    "$$\n",
    "P(\\text{fifth selection is red}) = \\sum_{i=0}^{2} P(\\text{fifth selection is red} \\mid X_4 = i) P(X_4 = i \\mid X_0 = 2)\n",
    "$$\n",
    "$$\n",
    "= (0) P_{2, 0}^4 + (0.5) P_{2, 1}^4 + (1) P_{2, 2}^4\n",
    "$$\n",
    "$$\n",
    "= 0.5 P_{2, 1}^4 + P_{2, 2}^4\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4872, 0.4352, 0.0776],\n",
       "       [0.2176, 0.5648, 0.2176],\n",
       "       [0.0776, 0.4352, 0.4872]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7048000000000001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.array([[0.8, 0.2, 0], [0.1, 0.8, 0.1], [0, 0.2, 0.8]])\n",
    "p_squared = np.matmul(p, p)\n",
    "p_quad = np.matmul(p_squared, p_squared)\n",
    "display(p_quad)\n",
    "\n",
    "0.5*p_quad[2][1] + p_quad[2][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4.11:**. Suppose that balls are successively distributed among 8 urns, with each ball being equally likely to be put in any of these urns. What is the probability that there will be exactly 3 nonempty urns after 9 balls have been distributed?\n",
    "\n",
    "- $X_n$ is the number of nonempty urns after $n$ balls have been distributed, then $X_n, n \\geq 0$ is a Markov chain with states $0, 1, 2, /dots, 8$ and transition probabilities: $P_{i,i} = i/8 = 1 - P_{i,i+1}, \\;\\; i = 0, 1, \\dots, 8$\n",
    "\n",
    "$$\n",
    "\\textbf{P} = \\begin{bmatrix}\n",
    "1/8 & 7/8 & 0 & 0 \\\\ \n",
    "0 & 2/8 & 6/8 & 0 \\\\ \n",
    "0 & 0 & 3/8 & 5/8 \\\\ \n",
    "0 & 0 & 0 & 1 \\\\ \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.96046448e-08, 1.06394291e-04, 7.57277012e-03, 9.92320776e-01],\n",
       "       [0.00000000e+00, 1.52587891e-05, 2.25484371e-03, 9.97729897e-01],\n",
       "       [0.00000000e+00, 0.00000000e+00, 3.91066074e-04, 9.99608934e-01],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.007572770118713379"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.array([[1/8, 7/8, 0, 0], [0, 2/8, 6/8, 0], [0, 0, 3/8, 5/8], [0, 0, 0, 1]])\n",
    "p_squared = np.matmul(p, p)\n",
    "p_quad = np.matmul(p_squared, p_squared)\n",
    "p_eight = np.matmul(p_quad, p_quad)\n",
    "display(p_eight)\n",
    "p_eight[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4.12:** In a sequence of independent flips of a fair coin, let $N$ denote the number of flips until there is a run of three consecutive heads. Find \\\n",
    "  a) $P(N \\leq 8)$ \\\n",
    "  b) $P(N = 8)$\n",
    "\n",
    "  <br>\n",
    "\n",
    "a)\n",
    "Define a Markov chain with states 0, 1, 2, 3 where for i < 3 state i means that we currently are on a run of i consecutive heads, and where state 3 means that a run of three consecutive heads has already occurred:\n",
    "\n",
    "$$\n",
    "\\textbf{P} = \\begin{bmatrix}\n",
    "1/2 & 1/2 & 0 & 0 \\\\ \n",
    "1/2 & 0 & 1/2 & 0 \\\\ \n",
    "1/2 & 0 & 0 & 1/2 \\\\ \n",
    "0 & 0 & 0 & 1 \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31640625, 0.171875  , 0.09375   , 0.41796875],\n",
       "       [0.265625  , 0.14453125, 0.078125  , 0.51171875],\n",
       "       [0.171875  , 0.09375   , 0.05078125, 0.68359375],\n",
       "       [0.        , 0.        , 0.        , 1.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.41796875"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.array([[1/2, 1/2, 0, 0], [1/2, 0, 1/2, 0], [1/2, 0, 0, 1/2], [0, 0, 0, 1]])\n",
    "p_squared = np.matmul(p, p)\n",
    "p_quad = np.matmul(p_squared, p_squared)\n",
    "p_eight = np.matmul(p_quad, p_quad)\n",
    "display(p_eight)\n",
    "p_eight[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07004693, 0.03808375, 0.02070571, 0.87116361],\n",
       "       [0.05878946, 0.03196318, 0.01737803, 0.89186932],\n",
       "       [0.03808375, 0.02070571, 0.01125747, 0.92995307],\n",
       "       [0.        , 0.        , 0.        , 1.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.010352857410907745"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_six = np.matmul(p_quad, p, p)\n",
    "p_seven = np.matmul(p_six, p)\n",
    "display(p_seven)\n",
    "1/2*p_seven[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.3 Classification of States**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.4 Long-Run Proportions and Limiting Probabilities**\n",
    "\n",
    "For pairs of states $ i \\ne j$, left $f_{i,j}$ denote the probability that the Markov chain, starting in state $i$ will ever make a transition into state $j$. That is, \n",
    "\n",
    "$$\n",
    "f_{i,j} = P(X_n = j \\text{ for some } n > 0 \\mid X_0 = i)\n",
    "$$\n",
    "\n",
    "**Proposition 4.3:** If $i$ is recurrent and $i$ communicates with $j$, then $f_{i,j} = 1$ \\\n",
    "\n",
    "**Definition:** Say that the recurrent state $j$ is **positive recurrent** if $m_j < \\infty$ and say that it is null recurrent if $m_j = \\infty$ \\\n",
    "\n",
    "**Proposition 4.4:** If the Markov chain is irreducible and recurrent, then for any initial state: $\\pi_j = \\frac{1}{m_j}$ \\\n",
    "\n",
    "**Proposition 4.5:** If $i$ is positive recurrent and $i \\xleftrightarrow{} j$ then $j$ is positive recurrent\n",
    "\n",
    "**Theorem 4.1:** Consider an irreducible Markov chain. If the chain is positive recurrent, then the long-run proportions are the unique solution of the equations:\n",
    "\n",
    "$$\n",
    "\\pi_j = \\sum_{i} \\pi_i P_{i,j}, \\quad j \\geq 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{j} \\pi_j = 1\n",
    "$$\n",
    "\n",
    "- there is no solution then Markov chain is either trasient or null recurrent all all $\\pi_j = 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4.22**: Consider Example 4.1, in which we assume that if it rains today, then it will rain tomorrow with probability α; and if it does not rain today, then it will rain tomorrow with probability β:\n",
    "\n",
    "$$\n",
    "\\textbf{P} = \\begin{bmatrix}\n",
    "\\alpha & 1 - \\alpha \\\\\n",
    "\\beta & 1 - \\beta \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\pi_0 = \\alpha \\pi_0 + \\beta \\pi_1,\n",
    "$$\n",
    "$$\n",
    "\\pi_1 = (1 - \\alpha) \\pi_0 + (1 - \\beta) \\pi_1,\n",
    "$$\n",
    "$$\n",
    "\\pi_0 + \\pi_1 = 1,\n",
    "$$\n",
    "$$\n",
    "\\pi_0 = \\frac{\\beta}{1 + \\beta - \\alpha}, \\quad \\pi_1 = \\frac{1 - \\alpha}{1 + \\beta - \\alpha}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4.23:** Consider Example 4.3 in which the mood of an individual is considered as a three-state Markov chain having a transition probability matrix\n",
    "\n",
    "$$\n",
    "\\textbf{P} = \\begin{bmatrix}\n",
    "0.5 & 0.4 & 0.1 \\\\ \n",
    "0.3 & 0.4 & 0.3 \\\\ \n",
    "0.2 & 0.3 & 0.5 \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In the long run, what proportion of time is the process in each of the three states?\n",
    "\n",
    "- long run proportions $π_i, \\; i = 0, 1, 2$ obtained from solution to system of eqs:\n",
    "\n",
    "$$\n",
    "\\pi_0 = 0.5 \\pi_0 + 0.3 \\pi_1 + 0.2 \\pi_2,\n",
    "$$\n",
    "$$\n",
    "\\pi_1 = 0.4 \\pi_0 + 0.4 \\pi_1 + 0.3 \\pi_2,\n",
    "$$\n",
    "$$\n",
    "\\pi_2 = 0.1 \\pi_0 + 0.3 \\pi_1 + 0.5 \\pi_2,\n",
    "$$\n",
    "$$\n",
    "\\pi_0 + \\pi_1 + \\pi_2 = 1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33870968, 0.37096774, 0.29032258])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = np.array([[0.5, 0.4, 0.1], [0.3, 0.4, 0.3], [0.2, 0.3, 0.5]])\n",
    "size = P.shape[0]\n",
    "A = np.eye(size) - P.T + np.ones((size, size))\n",
    "b = np.ones(size)\n",
    "stationary_distribution = np.linalg.solve(A, b)\n",
    "stationary_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4.24:** (A Model of Class Mobility). A problem of interest to sociologists is to determine the proportion of society that has an upper- or lower-class occupation. One possible mathematical model would be to assume that transitions between social classes of the successive generations in a family can be regarded as transitions of a Markov chain. That is, we assume that the occupation of a child depends only on his or her parent’s occupation. Let us suppose that such a model is appropriate and that the transition probability matrix is given by\n",
    "\n",
    "$$\n",
    "\\textbf{P} = \\begin{bmatrix}\n",
    "0.45 & 0.48 & 0.07 \\\\ \n",
    "0.05 & 0.70 & 0.25 \\\\ \n",
    "0.01 & 0.50 & 0.49 \\\\ \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06238859, 0.62344029, 0.31417112])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = np.array([[0.45, 0.48, 0.07], [0.05, 0.7, 0.25], [0.01, 0.50, 0.49]])\n",
    "size = P.shape[0]\n",
    "A = np.eye(size) - P.T + np.ones((size, size))\n",
    "b = np.ones(size)\n",
    "stationary_distribution = np.linalg.solve(A, b)\n",
    "stationary_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposition 4.6.** Let $\\{X_n, n \\geq 1\\}$ be an irreducible Markov chain with stationary probabilities $\\pi_j, j \\geq 0$, and let $r$ be a bounded function on the state space. Then, with probability 1,\n",
    "$$\n",
    "\\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{n=1}^N r(X_n) = \\sum_{j=0}^\\infty r(j) \\pi_j\n",
    "$$\n",
    "**Proof.** If we let $a_j(N)$ be the amount of time the Markov chain spends in state $j$ during time periods 1,...,N, then\n",
    "$$\n",
    "\\sum_{n=1}^N r(X_n) = \\sum_{j=0}^\\infty a_j(N) r(j)\n",
    "$$\n",
    "Since $\\frac{a_j(N)}{N} \\to \\pi_j$ the result follows from the preceding upon dividing by $N$ and then letting $N \\to \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***4.4.1 Limiting Probabilities***\n",
    "- An irreducible, positive recurrent, aperiodic Markov chain is said to be ergodic\n",
    "  - it is not always true that the long-run proportions are also limiting probabilities\n",
    "  - chain is said to be periodic if it does not have limiting probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.5 Some Applications**\n",
    "### ***4.5.1 The Gambler’s Ruin Problem***\n",
    "\n",
    "Consider a gambler who at each play of the game has probability $p$ of winning one unit and probability $q = 1 − p$ of losing one unit. Assuming that successive plays of the game are independent, what is the probability that, starting with $i$ units, the gambler’s fortune will reach $N$ before reaching 0?\n",
    "\n",
    "<style>\n",
    "    .math-large {\n",
    "        font-size: 135%; \n",
    "    }\n",
    "</style>\n",
    "<div class=\"math-large\">\n",
    "\n",
    "$$\n",
    "P_i = \n",
    "\\begin{cases} \n",
    "\\frac{1 - \\left(\\frac{q}{p}\\right)^i}{1 - \\left(\\frac{q}{p}\\right)^N}, & \\text{if } p \\neq \\frac{1}{2} \\\\\n",
    "\\frac{i}{N}, & \\text{if } p = \\frac{1}{2}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "As $N \\rightarrow \\infty $,\n",
    "\n",
    "$$\n",
    "P_i \\to \n",
    "\\begin{cases} \n",
    "1 - \\left(\\frac{q}{p}\\right)^i, & \\text{if } p > \\frac{1}{2} \\\\\n",
    "0, & \\text{if } p \\leq \\frac{1}{2}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4.30:** Suppose Max and Patty decide to flip pennies; the one coming closest to the wall wins. Patty, being the better player, has a probability 0.6 of winning on each flip. \\\n",
    ">> (a) If Patty starts with five pennies and Max with ten, what is the probability that Patty will wipe Max out? \\\n",
    ">> (b) What if Patty starts with 10 and Max with 20?\n",
    "\n",
    "a)\n",
    "- $i = 5$\n",
    "- $N = 15$\n",
    "- $p = 0.6$\n",
    "- $q = 0.4$\n",
    "\n",
    "$$\n",
    "\\frac{1 - \\left(\\frac{2}{3}\\right)^5}{1 - \\left(\\frac{2}{3}\\right)^{15}} \\approx 0.87\n",
    "$$\n",
    "\n",
    "\n",
    "b)\n",
    "- $i = 10$\n",
    "- $N = 30$\n",
    "- $p = 0.6$\n",
    "- $q = 0.4$\n",
    "\n",
    "$$\n",
    "\\frac{1 - \\left(\\frac{2}{3}\\right)^{10}}{1 - \\left(\\frac{2}{3}\\right)^{30}} \\approx 0.98\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
