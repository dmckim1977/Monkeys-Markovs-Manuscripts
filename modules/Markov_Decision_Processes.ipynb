{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes\n",
    "- Feldman/Valdez-Flores: Chapter 12, EXCLUDING: 12.3.1, 12.3.3, 12.4.2, 12.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **12.1 Basic Definitions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Example 12.1***: Let $X = \\{X_0, X_1, ...\\}$ be a a stochastic process with state space $E = \\{a, b, c, d \\}$. Process will represent a machine that can be in one of four operating conditions denoted by the states a through d indicating increasing levels of deterioration. As the machine deteriorates, not only is it more expensive to operate, but also production is lost. Standard maintenance activities are always carried out in states b through d so that the machine may improve due to maintenance; however, improvement is not guaranteed. Process has action space $A = \\{1, 2 \\}$ which gives the decisions possible at each step - at each step there are two possible actions: use an inexperienced operator (Action 1) or use an experienced operator (Action 2).\n",
    "\n",
    "Two cost vectors and two Markov matrices:\n",
    "\n",
    "$f_1 = (100,125,150,500)$\n",
    "\n",
    "$f_2 = (300,325,350,600)$\n",
    "\n",
    "$P_1 = \\begin{bmatrix}\n",
    "0.1 & 0.3 & 0.6 & 0.0 \\\\\n",
    "0.0 & 0.2 & 0.5 & 0.3 \\\\\n",
    "0.0 & 0.1 & 0.2 & 0.7 \\\\\n",
    "0.8 & 0.1 & 0.0 & 0.1 \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$P_2 = \\begin{bmatrix}\n",
    "0.6 & 0.3 & 0.1 & 0.0 \\\\\n",
    "0.75 & 0.1 & 0.1 & 0.05 \\\\\n",
    "0.8 & 0.2 & 0.0 & 0.0 \\\\\n",
    "0.9 & 0.1 & 0.0 & 0.0 \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "- if, at time $n$, the process is in state $i$ and the decision $k$ is made, then a cost of $f_k(i)$ is incurred and the probability that the next state will be $j$ is given by $P_k(i, j)$\n",
    "- if $X_n = a$ and decision 1 is made, then a cost of 100 is incurred (representing the operator cost, lost production cost, and machine operation cost) and $Pr\\{X_n+1 = a\\} = 0.1$\n",
    "- if $X_n = d$ and decision 2 is made, then a cost of 600 is incurred (representing the operator cost, machine operation cost, major maintenance cost, and lost-production cost) and $Pr\\{X_n+1 = a\\} = 0.9$\n",
    "\n",
    "\n",
    "$$\n",
    "X_n = i \\implies D_n = k \\implies f_k(i) \\implies P_k(i, j)\n",
    "$$\n",
    "$$\n",
    "\\text{Observe state} \\implies \\text{Take action} \\implies \\text{Incur cost} \\implies \\text{Transition to next state}\n",
    "$$\n",
    "\n",
    "- $X = \\{X_0, X_1, ...\\}$ is the system description process\n",
    "- $D = \\{D_0, D_1, ...\\}$ is the decision process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Definition 12.1:*** Let $X$ be a system description process with state space $E$ and let $D$ be a decision process with action space A. The process $(X,D)$ is a Markov Decision Process if, for $j \\in E$, and $n = 0,1, ··· ,$ the following holds\n",
    "\n",
    "$$\n",
    "\\Pr\\{X_{n+1} = j \\mid X_0, D_0, \\dots, X_n, D_n\\} = \\Pr\\{X_{n+1} = j \\mid X_n, D_n\\}\n",
    "$$\n",
    "\n",
    "Futhermore, for each $k \\in A$, let $\\textbf{f}_k$ be the cost vector and $\\textbf{P}_k$ be a Markov matrix. Then, \n",
    "\n",
    "$$\n",
    "\\Pr\\{X_{n+1} = j \\mid X_n = i, D_n = k\\} = P_k(i, j)\n",
    "$$\n",
    "\n",
    "and the cost $f_k(i)$ is incurred whenever $X_n = i$ and $D_n = k$\\\n",
    "\n",
    "- Guiding Question: \"How can decisions be made as to minimize costs?\" - \"What do we mean by minimize?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Definition 12.2:*** A policy is any rule, using current information, past information, and/or randomization that specifies which action to take at each point in time. The set of all (decision) policies is denoted by $\\mathcal{D}$\n",
    "\n",
    "Examples of policies from Example 12.1:\n",
    "- **Policy 1:** Always choose action 1, independent of the state for $X$, i.e., let $D_n \\equiv 1$ for all $n$.\n",
    "- **Policy 2:** If $X_n$ is in state $a$ or $b$, let $D_n = 1$; if $X_n$ is in state $c$ or $d$, let $D_n = 2$.\n",
    "- **Policy 3:** If $X_n$ is in state $a$ or $b$, let $D_n = 1$; if $X_n$ is in state $c$, toss a (fair) coin and let $D_n = 1$ if the toss results in a head and let $D_n = 2$ if it results in a tail; if $X_n$ is in state $d$, let $D_n = 2$.\n",
    "- **Policy 4:** Let $D_n \\equiv 1$ for $n = 0$ and $1$. For $n \\geq 2$, if $X_n > X_{n-1}$ and $X_{n-2} = a$, let $D_n = 1$; if $X_n > X_{n-1}$, $X_{n-2} = b$, and $D_{n-1} = 2$ let $D_n = 1$; otherwise, let $D_n = 2$.\n",
    "\n",
    "<br>\n",
    "\n",
    "- The Markov decision process is not necessarily a Markov chain, because we can allow decisions to depend upon history\n",
    "\n",
    "<br>\n",
    "\n",
    "Minimization criteria used: \n",
    "- (1) expected total discounted cost\n",
    "- (2) average long-run cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***12.1.1 Expected Total Discounted Cost Criterion***\n",
    "\n",
    "- Equivalent to using a present worth calculation for the basis of decision making\n",
    "  - Bond pricing, DCF, etc\n",
    "- The expected total discounted cost for a particular Markov decision process: $E[\\sum_{n=0}^{\\infty} \\alpha^n \\mathcal{f}_{D_n}(X_n)]$\n",
    "  - at time $n = 1$ has a present value of $\\alpha$ at time $n = 0$\n",
    "  - $\\alpha = 1/(1+r))$\n",
    "\n",
    "Example: From example 12.1, Policy 1 is chosen (i.e., the inexperienced operator is always used), and a discount factor of $\\alpha = 0.95$ (equivalent to a rate of return of approximately 5.3% per period) is used. Then this reduces to the computation of the total discounted cost of a standard Markov chain. Expected total discounted cost is gived by $(\\textbf{I} - \\alpha \\textbf{P}_1)^{-1} \\textbf{f}_1$ $\\implies$ $\\textbf{v} = (4502,4591,4676,4815)^T$ $\\implies$ if the process starts in state $a$, the expected present value of all future costs is 4502.\n",
    "\n",
    "\n",
    "There exists a dependence on the specific policy before expectations can be taken. Subscript with the expectation operator denotes an expectation under the probability law specified by the policy $d \\in \\mathcal{D}$. The total discounted value of a Markov decision process under a discount factor of $\\alpha$ using the policy $d \\in \\mathcal{D}$ will be denoted by $v_d^{\\alpha}$:\n",
    "\n",
    "$$\n",
    "v_d^{\\alpha}(i) = E_d \\Bigg[\\sum_{n=0}^{\\infty} \\alpha^n \\mathcal{f}_{D_n}(X_n) \\mid X_0 = i \\Bigg]\n",
    "$$\n",
    "\n",
    "for $i \\in E$ and $0 < \\alpha < 1$\n",
    "\n",
    "The discounted cost optimization problem can be stated as: Find $d^{\\alpha} \\in \\mathcal{D}$ such that $v_{d^\\alpha}^{\\alpha(i)} = v^{\\alpha}(i)$ where the vector $\\textbf{v}^{\\alpha}$ is defined for $i \\in E$ by\n",
    "\n",
    "$$\n",
    "v^{\\alpha}(i) = \\min_{d \\in \\mathcal{D}} v_d^{\\alpha}(i)\n",
    "$$\n",
    "\n",
    "- the existence of an optimal policy can be a difficult question when the state space is infinite\n",
    "- we shall only consider problems in which its existence is assured by assuming that both the state space and action space are finite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***12.1.2 Average Long-Run Cost Criterion***\n",
    "\n",
    "- Using an infinite horizon planning period, the total (undiscounted) cost may be infinite for all possible decisions so that total cost cannot be used to distinguish between alternative policies.\n",
    "- We can use cost per transition for comparison allowing alternatives to be evaluated for an infinite horizon planning period\n",
    "- A commonly used criterion: $\\lim_{m \\to \\infty} \\frac{1}{m} \\sum_{n=0}^{m-1} f_{D_n}(X_n)$\n",
    "- For example, assume that Policy 1 is used; thus, Action 1 is always chosen.\n",
    "  - long-run cost can be calculated by the steady-state probabilities using the matrix $\\textbf{P}_1$ which gives us $\\textbf{π} = (0.253,0.167,0.295,0.285)$\n",
    "  - $\\textbf{π}$ $\\cdot$ $\\textbf{f}_1$ $= 232.925$\n",
    "\n",
    "For a fixed policy $d \\in \\mathcal{D}$ the average long-run cost for the Markov decision process will be denoted by $\\phi_d$:\n",
    "\n",
    "$$\n",
    "\\phi_d = \\lim_{m \\to \\infty} \\frac{f_{D_0}(X_0) + \\cdots + f_{D_{m-1}}(X_{m-1})}{m}\n",
    "$$\n",
    "\n",
    "The optimization problem can be stated as: Find $d^* \\in \\mathcal{D}$ such that $\\phi_{d^*} = \\phi^*$, where $\\phi^*$ is defined by:\n",
    "$$\n",
    "\\phi^* = \\min_{d \\in \\mathcal{D}} \\phi_d\n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **12.2 Stationary Policies**\n",
    "\n",
    "**Definition 12.3:** An action function is a vector which maps the state space into the action space, i.e., an action function assigns an action to each state.\n",
    "- if **a** is an action function, then $a(i) \\in A$ for each $i \\in E$. Policy 2 is equivalent to the action function $\\textbf{a} = (1,1,2,2)$, where the action space is $A = \\{ 1,2 \\}$.\n",
    "\n",
    "**Definition 12.4:** A stationary policy is a policy that can be defined by an action function. The stationary policy defined by the function a takes action $a(i)$ at time $n$ if $X_n = i$, independent of previous states, previous actions, and time $n$.\n",
    "- stationary policy is independent of time\n",
    "- stationary policy is a non-randomized policy that only depends on the current state of the process and thus ignores history\n",
    "- it is a Markov decision process under a stationary policy is always a Markov chain\n",
    "\n",
    "**Property 12.1**. If the state space E is finite, there exists a stationary policy that solves the problem given in Eq. (12.1). Furthermore, if every stationary policy yields an irreducible Markov chain, there exists a stationary policy that solves the problem given in Eq. (12.2). (The optimum policy may depend on the discount factor and may be different for Eqs. (12.1) and (12.2).)\n",
    "- Eq 12.1 is Expected Total Discounted Cost Criterion optimization problem\n",
    "- Eq 12.2 is Average Long-Run Cost Criterion optimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **12.3 Discounted Cost Algorithms**\n",
    "- Based on a fixed-point property that holds for the optimal value function\n",
    "- a function is called invariant with respect to an operation if the operation does not vary the function\n",
    "  - steady-state vector **π** is the invariant vector for the Markov matrix **P** b/c **πP** does not vary the vector **π**\n",
    "  - the invariant property as applied to Markov decision processes will be recognized as the standard dynamic programming recursive relationship\n",
    "  - involves minimizing current costs plus all future costs, where the future costs must be discounted to the present in order to produce a total present value\n",
    "\n",
    "**Property 12.2. Fixed-Point Theorem for Markov Decision Processes:** \\\n",
    "$\\textbf{v}^a$ is the  optimal value function of the Expected Total Discounted Cost Criterion optimization problem with $0 < \\alpha < 1$. The function $\\textbf{v}^a$ satisfies, for each $i \\in E$ the following:\n",
    "$$\n",
    "\\textbf{v}^a(i) = \\min_{k \\in A}\\{f_k(i) + \\alpha \\sum_{j \\in E} P_k(i, j)v^{\\alpha}(j) \\}\n",
    "$$\n",
    "\n",
    "- it is the only function satisfying this property\n",
    "- provides a means to determine if a given function happens to be the optimal function\n",
    "\n",
    "**Property 12.3:** $\\textbf{v}^a$ is the  optimal value function of the Expected Total Discounted Cost Criterion optimization problem with $0 < \\alpha < 1$. Define an action function for each $i \\in E$ the following:\n",
    "$$\n",
    "a(i) = \\underset{k \\in A}{\\mathrm{argmin}} \\{f_k(i) + \\alpha \\sum_{j \\in E} P_k(i, j)v^{\\alpha}(j) \\}\n",
    "$$\n",
    "\n",
    "- The stationary policy defined by the action function **a** is an optimal policy.\n",
    "- Property 12.3 tells how to obtain the optimal policy once $v^\\alpha$ is known (we dont know how to get $v^\\alpha$ yet)\n",
    "\n",
    "**Example:** The optimal value function for the machine problem given in Example 12.1 is $\\textbf{v}^\\alpha = (4287,4382,4441,4613)$ for $\\alpha = 0.95$\n",
    "- verify the assertion through the Fixed-Point Theorem for Markov Decision Processes\n",
    "  - $v^{\\alpha}(a)$ = $\\min{\\{f_1[0] + 0.95 * P_1[0] \\times \\textbf{v}^\\alpha, f_2[0] + 0.95 * P_2[0] \\times \\textbf{v}^\\alpha \\}} $\n",
    "  - $v^{\\alpha}(b)$ = $\\min{\\{f_1[1] + 0.95 * P_1[1] \\times \\textbf{v}^\\alpha, f_2[1] + 0.95 * P_2[1] \\times \\textbf{v}^\\alpha \\}} $\n",
    "  - $v^{\\alpha}(c)$ = $\\min{\\{f_1[2] + 0.95 * P_1[2] \\times \\textbf{v}^\\alpha, f_2[2] + 0.95 * P_2[2] \\times \\textbf{v}^\\alpha \\}} $\n",
    "  - $v^{\\alpha}(d)$ = $\\min{\\{f_1[3] + 0.95 * P_1[3] \\times \\textbf{v}^\\alpha, f_2[3] + 0.95 * P_2[3] \\times \\textbf{v}^\\alpha \\}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = np.array([100, 125, 150, 500])\n",
    "f2 = np.array([300, 325, 350, 600])\n",
    "\n",
    "p1 = np.matrix(\n",
    "    [\n",
    "        [0.1, 0.3, 0.6, 0.0],\n",
    "        [0.0, 0.2, 0.5, 0.3],\n",
    "        [0.0, 0.1, 0.2, 0.7],\n",
    "        [0.8, 0.1, 0.0, 0.1],\n",
    "    ]\n",
    ")\n",
    "p2 = np.matrix(\n",
    "    [\n",
    "        [0.6, 0.3, 0.1, 0.0],\n",
    "        [0.75, 0.1, 0.1, 0.05],\n",
    "        [0.8, 0.2, 0.0, 0.0],\n",
    "        [0.9, 0.1, 0.0, 0.0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "v_super_alpha = np.array([4287, 4382, 4441, 4613])\n",
    "\n",
    "alpha = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_6036\\3814733425.py:3: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  action_1 = (float(f1[i] + alpha * np.dot(p1[i], v_super_alpha)[0]), \"action1\", i+1)\n",
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_6036\\3814733425.py:4: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  action_2 = (float(f2[i] + alpha * np.dot(p2[i], v_super_alpha)[0]), \"action2\", i+1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(4287.504999999999, 'action1', 1),\n",
       " (4381.76, 'action1', 2),\n",
       " (4440.7, 'action2', 3),\n",
       " (4612.645, 'action1', 4)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_values = []\n",
    "for i in range(0, 4):\n",
    "    action_1 = (float(f1[i] + alpha * np.dot(p1[i], v_super_alpha)[0]), \"action1\", i+1)\n",
    "    action_2 = (float(f2[i] + alpha * np.dot(p2[i], v_super_alpha)[0]), \"action2\", i+1)\n",
    "    optimal_values.append(action_1 if action_1[0] < action_2[0] else action_2)\n",
    "\n",
    "optimal_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since, for each $i \\in E$, the minimum of the two values yielded the asserted value of $v^\\alpha(i)$, we know that it is optimum by Property 12.2\n",
    "- Can also pick out the argument (i.e., action) that resulted in the minimum value\n",
    "  - State $i = a$, the first action yielded the minimum\n",
    "  - State $i = b$, the first action yielded the minimum\n",
    "  - State $i = c$, the second action yielded the minimum\n",
    "  - State $i = d$, the first action yielded the minimum\n",
    "- Therefore, from Property 12.3, the stationary optimal policy is defined by the action function $\\textbf{a} = (1,1,2,1).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **12.3.2 Policy Improvement for Discounted Costs**\n",
    "- An algorithm that focuses on the policy and then calculates the value associated with that particular policy\n",
    "- Result is that convergence is significantly faster, but there are more calculations for each iteration say compared to a Value Improvement for Discounted Costs algo\n",
    "\n",
    "**Property 12.5. Policy Improvement Algorithm:** The following iteration procedure will yield the optimal value function as defined by Eq 12.1 (Expected Total Discounted Cost Criterion optimization problem) and its associated optimal stationary policy:\n",
    "\n",
    "**Step 1.** Make sure that $\\alpha < 1$, set $n = 0$, and define the action function $a_0$ by:\n",
    "$$\n",
    "a_0(i) = \\arg\\min_{k \\in A} f_k(i)\n",
    "$$\n",
    "for each $i \\in E$.\n",
    "\n",
    "**Step 2.** Define the matrix $P$ and the vector $f$ by:\n",
    "$$\n",
    "f(i) = f_{a_n(i)}(i)\n",
    "$$\n",
    "$$\n",
    "P(i, j) = P_{a_n(i)}(i, j)\n",
    "$$\n",
    "for each $i, j \\in E$.\n",
    "\n",
    "**Step 3.** Define the value function $v$ by:\n",
    "$$\n",
    "v = (I - \\alpha P)^{-1}f\n",
    "$$\n",
    "\n",
    "**Step 4.** Define the action function $a_{n+1}$ by:\n",
    "$$\n",
    "a_{n+1}(i) = \\arg\\min_{k \\in A} \\left\\{ f_k(i) + \\alpha \\sum_{j \\in E} P_k(i, j) v(j) \\right\\}\n",
    "$$\n",
    "for each $i \\in E$.\n",
    "\n",
    "**Step 5.** If $a_{n+1} = a_n$, let $v_\\alpha = v$, $a_\\alpha = a_n$, and stop; otherwise, increment $n$ by one and return to Step 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement_for_discounted_costs_algoritm(fs, Ps, alpha, num_iterations=100):\n",
    "    num_states = len(fs[0])\n",
    "    # Initial policy\n",
    "    current_policy = np.zeros(num_states, dtype=int)\n",
    "    for _ in range(num_iterations):\n",
    "        f = np.array([fs[current_policy[i]][i] for i in range(num_states)])\n",
    "        P = np.vstack([Ps[current_policy[i]][i, :] for i in range(num_states)])\n",
    "        v = np.linalg.inv(np.eye(num_states) - alpha * P) @ f\n",
    "\n",
    "        # Policy improvement\n",
    "        new_policy = np.zeros_like(current_policy)\n",
    "        for i in range(num_states):\n",
    "            # Evaluate each action's value at state i\n",
    "            # Correcting the dimension of v\n",
    "            q_values = [fs[a][i] + alpha * (Ps[a][i, :] @ v.T) for a in range(len(fs))]\n",
    "            # Update policy\n",
    "            new_policy[i] = np.argmin(q_values)\n",
    "\n",
    "        # Check for convergence (if policy does not change)\n",
    "        if np.array_equal(new_policy, current_policy):\n",
    "            return v, current_policy\n",
    "        current_policy = new_policy\n",
    "\n",
    "    return v, current_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(matrix([[4287.40288177, 4381.63406971, 4440.93666339, 4612.90765388]]),\n",
       " array([0, 0, 1, 0]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = [f1, f2]\n",
    "Ps = [p1, p2]\n",
    "\n",
    "optimal_value, optimal_policy = policy_improvement_for_discounted_costs_algoritm(fs, Ps, alpha)\n",
    "optimal_value, optimal_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **12.4 Average Cost Algorithms**\n",
    "- Average cost criterion problem is slightly more difficult than the discounted cost criterion problem because it was the discount factor that produced a fixed-point\n",
    "- A recursive equation that is analogous to Property 12.2 as follows:\n",
    "\n",
    "**Property 12.8.** Assume that every stationary policy yields a Markov chain with only one irreducible set. There exists a scalar $\\phi^*$ and a vector $h$ such that, for all $i \\in E$,\n",
    "\n",
    "$$\n",
    "\\phi^* + h(i) = \\min_{k \\in A} \\left\\{ f_k(i) + \\sum_{j \\in E} P_k(i, j) h(j) \\right\\}.\n",
    "$$\n",
    "\n",
    "The scalar $\\phi^*$ is the optimal cost as defined by Eq. 12.2 (Average Long-Run Cost Criterion optimization problem), and the optimal action function is defined by\n",
    "\n",
    "$$\n",
    "a(i) = \\arg\\min_{k \\in A} \\left\\{ f_k(i) + \\sum_{j \\in E} P_k(i, j) h(j) \\right\\}.\n",
    "$$\n",
    "\n",
    "The vector $\\textbf{h}$ is unique up to an additive constant.\n",
    "\n",
    "\n",
    "- use this property to determine if the optimal policy previously determined for a discount factor of 0.95 is also the optimal policy under the average cost criterion\n",
    "- would like to determine if the stationary policy defined by $\\textbf{a} = (1,1,2,1)$ is optimal using the long-run average cost criterion\n",
    "- solve:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\phi^* + h_a &= 100 + 0.1 h_a + 0.3 h_b + 0.6 h_c, \\\\\n",
    "\\phi^* + h_b &= 125 + 0.2 h_b + 0.5 h_c + 0.3 h_d, \\\\\n",
    "\\phi^* + h_c &= 350 + 0.8 h_a + 0.2 h_b, \\\\\n",
    "\\phi^* + h_d &= 500 + 0.8 h_a + 0.1 h_b + 0.1 h_d.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi*: 219.23774954627947\n",
      "vector h: [0.0, 97.09618874773139, 150.18148820326678, 322.746521476104]\n"
     ]
    }
   ],
   "source": [
    "A = np.array(\n",
    "    [\n",
    "        [1, -0.3, -0.6, 0],  # From eq1\n",
    "        [1, 0.8, -0.5, -0.3],  # From eq2\n",
    "        [1, -0.2, 1, 0],  # From eq3\n",
    "        [1, -0.1, 0, 0.9],  # From eq4\n",
    "    ]\n",
    ")\n",
    "\n",
    "B = np.array([100, 125, 350, 500])\n",
    "\n",
    "solution = np.linalg.solve(A, B)\n",
    "phi_star = solution[0]\n",
    "h = np.append([0], solution[1:]).tolist()\n",
    "\n",
    "print(\"phi*:\", phi_star)\n",
    "print(\"vector h:\", h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- these values have been determined so that for each $k = a(i)$ so $\\phi^* + h(i) = f_k(i) + \\textbf{P}_\\textbf{k} \\textbf{h}(i)$\n",
    "- to determine optimality, we must verify that for each $k \\neq a(i)$ so $\\phi^* + h(i) \\leq f_k(i) + \\textbf{P}_\\textbf{k} \\textbf{h}(i)$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\phi^* + h_a &\\leq 300 + 0.6 h_a + 0.3 h_b + 0.1 h_c, \\\\\n",
    "\\phi^* + h_b &\\leq 325 + 0.75 h_a + 0.1 h_b + 0.1 h_c + 0.05 h_d, \\\\\n",
    "\\phi^* + h_c &\\leq 150 + 0.1 h_b + 0.2 h_c + 0.7 h_d, \\\\\n",
    "\\phi^* + h_d &\\leq 600 + 0.9 h_a + 0.1 h_b.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[219.23774955 316.33393829 369.41923775 541.98427102]\n",
      "[344.1470054446461, 365.865093768905, 415.6684815486993, 609.7096188747731]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients = np.array([\n",
    "    [0.6, 0.3, 0.1, 0.0],  # Coefficients for h_a, h_b, h_c, h_d in the first inequality\n",
    "    [0.75, 0.1, 0.1, 0.05], # Coefficients for h_a, h_b, h_c, h_d in the second inequality\n",
    "    [0.0, 0.1, 0.2, 0.7],   # Coefficients for h_a, h_b, h_c, h_d in the third inequality\n",
    "    [0.9, 0.1, 0.0, 0.0]    # Coefficients for h_a, h_b, h_c, h_d in the fourth inequality\n",
    "])\n",
    "constants = [300, 325, 150, 600]\n",
    "\n",
    "lhs = phi_star + h\n",
    "rhs = [constants[i] + np.dot(h, co) for i, co in enumerate(coefficients)]\n",
    "print(lhs)\n",
    "print(rhs)\n",
    "inequalities_hold = np.all(lhs <= rhs)\n",
    "inequalities_hold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- once the optimal policy is known, the value $\\phi^*$ can be obtained by the long-run probabilities dot cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row sums: [1. 1. 1. 1.]\n",
      "[0.36297641 0.22867514 0.33212341 0.07622505]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "219.2377495462795"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_super_a = np.array([\n",
    "    [0.1, 0.3, 0.6, 0.0],\n",
    "    [0.0, 0.2, 0.5, 0.3],\n",
    "    [0.8, 0.2, 0.0, 0.0],\n",
    "    [0.8, 0.1, 0.0, 0.1]\n",
    "])\n",
    "row_sums = P_super_a.sum(axis=1)\n",
    "print(\"Row sums:\", row_sums)\n",
    "\n",
    "# Solve for the stationary distribution of the Markov chain\n",
    "# Create the matrix (I - P^T + 1v^T) where 1 is a column vector of all ones and v is a row vector of all ones\n",
    "size = P_super_a.shape[0]\n",
    "A = np.eye(size) - P_super_a.T + np.ones((size, size))\n",
    "b = np.ones(size)\n",
    "\n",
    "# Solve the linear system to find the stationary distribution\n",
    "stationary_distribution = np.linalg.solve(A, b)\n",
    "print(stationary_distribution)\n",
    "\n",
    "cost_vector_g = np.array([100, 125, 350, 500])\n",
    "\n",
    "np.dot(stationary_distribution, cost_vector_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](images/feldman_flores_table12.1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value for each component in the vector $(1 - \\alpha)v_\\alpha$ approaches $\\phi^*$ as the discount factor approaches one. To understand this, remember that the geometric series is given by:\n",
    "\n",
    "$$\n",
    "\\sum_{n=0}^\\infty \\alpha^n = \\frac{1}{1 - \\alpha};\n",
    "$$\n",
    "\n",
    "thus, if a cost of $c$ is incurred every period with a discount factor of $\\alpha$, its total value would be:\n",
    "\n",
    "$$\n",
    "v = \\frac{c}{1 - \\alpha}.\n",
    "$$\n",
    "\n",
    "Or conversely, a total cost of $v$ is equivalent to an average per period cost of:\n",
    "\n",
    "$$\n",
    "c = (1 - \\alpha)v.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Property 12.9:**. Let $\\textbf{v}^\\alpha$ be the optimal value function defined by Eq 12.1 (Expected Total Discounted Cost Criterion optimization problem), let $\\phi^*$ be the optimal cost defined by Eq 12.2 (Average Long-Run Cost Criterion optimization problem), and assume that every stationary policy yields a Markov chain with only one irreducible set. Then\n",
    "\n",
    "$$\n",
    "\\lim_{\\alpha \\to 1} (1 - \\alpha)v^\\alpha(i) = \\phi^*\n",
    "$$\n",
    "\n",
    "for any $i \\in E$.\n",
    "\n",
    "**Property 12.10:** Let $\\textbf{v}^\\alpha$ be the optimal value function defined by Eq 12.1 (Expected Total Discounted Cost Criterion optimization problem), let $\\phi^*$ be the optimal cost defined by Eq 12.2 (Average Long-Run Cost Criterion optimization problem) and let $\\textbf{h}$ be the vector defined by Property 12.8. Then\n",
    "\n",
    "$$\n",
    "\\lim_{\\alpha \\to 1} v^\\alpha(i) - v^\\alpha(j) = h^\\alpha(i) - h^\\alpha(j)\n",
    "$$\n",
    "\n",
    "for any $i, j \\in E$.\n",
    "\n",
    "- This property is the justification for arbitrarily setting $h(a) = 0$ when we solve for $\\phi^∗$ and h. In fact, it is legitimate to pick any single state and set its $\\textbf{h}$ value equal to any given number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **12.4.1 Policy Improvement for Average Costs**\n",
    "\n",
    "- Begins with an arbitrary policy, determines the $\\phi^*$ and $h$ values associated with it, and then either establishes that the policy is optimal or produces a better policy\n",
    "\n",
    "**Property 12.11. Policy Improvement Algorithm:** The following iteration procedure will yield the optimal value function as defined by Eq 12.2 (Average Long-Run Cost Criterion optimization problem) and its associated optimal stationary policy:\n",
    "\n",
    "**Step 1.** Set $n = 0$, let the first state in the state space be denoted by the number 1, and define the action function $\\textbf{a}_0$ by:\n",
    "$$\n",
    "a_0(i) = \\arg\\min_{k \\in A} f_k(i)\n",
    "$$\n",
    "for each $i \\in E$.\n",
    "\n",
    "**Step 2.** Define the matrix $\\textbf{P}$ and the vector $\\textbf{f}$ by:\n",
    "$$\n",
    "f(i) = f_{a_n(i)}(i)\n",
    "$$\n",
    "$$\n",
    "P(i, j) = P_{a_n(i)}(i, j)\n",
    "$$\n",
    "for each $i, j \\in E$.\n",
    "\n",
    "**Step 3.** Determine values for $\\phi$ and $\\textbf{h}$ by solving the system of equations given by:\n",
    "$$\n",
    "\\phi + \\textbf{h} = \\textbf{f} + \\textbf{Ph},\n",
    "$$\n",
    "where $h(1) = 0$.\n",
    "\n",
    "**Step 4.** Define the action function $\\textbf{a}_{n+1}$ by:\n",
    "$$\n",
    "a_{n+1}(i) = \\arg\\min_{k \\in A} \\{ f_k(i) + \\sum_{j \\in E} P_k(i, j) h(j) \\}\n",
    "$$\n",
    "for each $i \\in E$.\n",
    "\n",
    "**Step 5.** If $\\textbf{a}_{n+1} = \\textbf{a}_n$, let $\\phi^* = \\phi$, $\\textbf{a}^* = \\textbf{a}_n$, and stop; otherwise, increment $n$ by one and return to Step 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement_for_average_costs_algoritm(fs, Ps):\n",
    "    n = 0\n",
    "    a_prev = [1,1,1,1]\n",
    "    while True:\n",
    "        P = Ps[n]\n",
    "        f = fs[n]\n",
    "        A = []\n",
    "        current_policy = np.zeros(len(f), dtype=int)\n",
    "        for i in range(0, len(f)):\n",
    "            curr_row = P[i]\n",
    "            new_row = [0] * len(f)\n",
    "            new_row[0] = 1\n",
    "            for j, val in enumerate(curr_row):\n",
    "                if j == 0:\n",
    "                    continue\n",
    "                if j == i:\n",
    "                    new_row[j] = (1 - val)\n",
    "                else:\n",
    "                    new_row[j] = -val\n",
    "                    \n",
    "            A.append(new_row)\n",
    "            \n",
    "        solution = np.linalg.solve(A, f)\n",
    "        phi_star = solution[0]\n",
    "        h = np.append([0], solution[1:])\n",
    "        \n",
    "        a_new_policy = np.zeros_like(current_policy) \n",
    "        for i in range(0, len(f)):\n",
    "            q_values = [fs[a][i] + (Ps[a][i, :] @ h.T) for a in range(len(fs))]\n",
    "            a_new_policy[i] = np.argmin(q_values) + 1\n",
    "\n",
    "        if np.array_equal(a_new_policy, a_prev):\n",
    "            return a_new_policy, phi_star, h\n",
    "        \n",
    "        a_prev = a_new_policy\n",
    "        n = n + 1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 2, 1]),\n",
       " 219.23774954627947,\n",
       " array([  0.        ,  97.09618875, 150.1814882 , 322.74652148]))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = np.array(\n",
    "    [\n",
    "        [0.1, 0.3, 0.6, 0.0],\n",
    "        [0.0, 0.2, 0.5, 0.3],\n",
    "        [0.0, 0.1, 0.2, 0.7],\n",
    "        [0.8, 0.1, 0.0, 0.1],\n",
    "    ]\n",
    ")\n",
    "p2 = np.array(\n",
    "    [\n",
    "        [0.1, 0.3, 0.6, 0.0],\n",
    "        [0.0, 0.2, 0.5, 0.3],\n",
    "        [0.8, 0.2, 0.0, 0.0],\n",
    "        [0.8, 0.1, 0.0, 0.1],\n",
    "    ]\n",
    ")\n",
    "Ps = [p1, p2]\n",
    "\n",
    "fs = [[100, 125, 150, 500], [100, 125, 350, 500]]\n",
    "\n",
    "policy_improvement_for_average_costs_algoritm(fs, Ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
